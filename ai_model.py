# -*- coding: utf-8 -*-
"""AI_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x8SHVzJBdD8zfQpUlRN85xDjiqHDVnOs

Install Required Libraries
"""

# !pip install -U datasets
# !pip install sentence-transformers    
# !pip install faiss-cpu
# !pip install nlpaug
# !pip install pdfplumber PyPDF2 pdf2image pytesseract

import logging
logging.getLogger("pdfminer").setLevel(logging.ERROR)

import pandas as pd
import os
import gc
import re
import numpy as np
import faiss
import pdfplumber
import json
import random
import hashlib
import spacy
import unicodedata

import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords

from sentence_transformers import SentenceTransformer

from typing import List, Tuple, Dict, Optional
import nlpaug.augmenter.word as naw
from datasets import load_dataset

import torch
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split

from transformers import (
    AutoTokenizer, AutoConfig,
    T5ForConditionalGeneration,
    get_linear_schedule_with_warmup, DataCollatorForSeq2Seq
)

from tqdm.auto import tqdm

# Mount Google Drive (if needed)
# from google.colab import drive
# drive.mount('/content/drive')

# Reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Download necessary NLTK data if not already present
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab')
nltk.download('stopwords')

try:
    nlp = spacy.load("en_core_web_sm")
except IOError:
    print("Downloading spaCy model...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

"""Load the Datasets"""

def load_openstax_textbooks(max_samples: int = None) -> pd.DataFrame:
    """
    Enhanced OpenStax textbook loader with better content quality filtering
    """
    print("Loading OpenStax textbooks for summarization task...")
    ds = load_dataset("crumb/openstax-text", split="train")
    records = []

    # Enhanced filtering criteria
    min_words = 15  # Increased minimum
    min_record_words = 25  # Higher threshold for meaningful content

    for ex in ds:
        text = ex.get("text", "")
        if not text or len(text.strip()) < min_words:
            continue

        # Enhanced paragraph processing
        paragraphs = [p.strip() for p in text.split('\n\n') if len(p.strip()) > 0]
        filtered = []

        for para in paragraphs:
            if (len(para.split()) >= min_words and
                # Must contain educational indicators
                any(pattern in para.lower() for pattern in [
                    'is', 'are', 'means', 'refers to', 'defined as', 'called', 'known as',
                    'process', 'method', 'technique', 'principle', 'concept', 'theory',
                    'function', 'purpose', 'role', 'allows', 'enables', 'provides'
                ]) and
                # Should not be mostly numbers or references
                len([w for w in para.split() if re.match(r'^[\d\.\-/]+$', w)]) / len(para.split()) < 0.3):
                filtered.append(para)

        # Create better summaries
        for para in filtered:
            sentences = sent_tokenize(para)
            if len(sentences) >= 3:  # Need substantial content
                # Create extractive summary from key sentences
                key_sentences = []
                for sent in sentences:
                    if any(indicator in sent.lower() for indicator in [
                        'important', 'key', 'main', 'primary', 'essential', 'significant',
                        'fundamental', 'critical', 'major', 'central'
                    ]):
                        key_sentences.append(sent)

                # If no key sentences found, use first 2 sentences
                if not key_sentences:
                    key_sentences = sentences[:2]

                summary = " ".join(key_sentences[:2])  # Limit to 2 sentences

                if (len(para.split()) >= min_record_words and
                    len(summary.split()) >= 8):  # Ensure substantial summary
                    records.append({
                        "text": para,
                        "ctext": summary,
                        "subject": "general",
                        "task": "summarization",
                        "domain": "education"
                    })

    # Enhanced sampling strategy
    if max_samples and len(records) > max_samples:
        # Stratified sampling to maintain diversity
        random.shuffle(records)
        records = records[:max_samples]

    df = pd.DataFrame(records)
    print(f"Loaded {len(df)} OpenStax summarization examples")
    return df

def load_mmlu_dataset(max_samples: int = None) -> pd.DataFrame:
    """
    IMPROVED: Fixed subject categorization and output formatting
    """
    print("Loading MMLU for MCQ GENERATION (IMPROVED FORMAT)...")
    try:
        subjects = [
            'anatomy', 'astronomy', 'college_biology', 'college_chemistry',
            'college_computer_science', 'college_mathematics', 'college_physics',
            'conceptual_physics', 'elementary_mathematics', 'formal_logic',
            'high_school_biology', 'high_school_chemistry', 'high_school_computer_science',
            'high_school_mathematics', 'high_school_physics', 'machine_learning',
            'medical_genetics', 'nutrition', 'philosophy', 'virology'
        ]

        sample_per_subject = max_samples // len(subjects) if max_samples else 5
        records = []

        for subject in tqdm(subjects, desc="Processing MMLU"):
            try:
                ds = load_dataset("cais/mmlu", subject, split="dev")

                if len(ds) > sample_per_subject:
                    indices = list(range(len(ds)))
                    random.shuffle(indices)
                    indices = indices[:sample_per_subject]
                    ds = ds.select(indices)

                for example in ds:
                    question = example.get("question", "").strip()
                    choices = example.get("choices", [])
                    answer_idx = example.get("answer", 0)

                    if question and choices and len(choices) >= 4:
                        correct_answer = choices[answer_idx].strip()
                        wrong_options = [choices[i].strip() for i in range(len(choices)) if i != answer_idx]

                        # FIXED: Better subject mapping logic
                        subject_category = "general"  # default
                        subject_lower = subject.lower()

                        if any(bio_term in subject_lower for bio_term in ['biology', 'anatomy', 'medical', 'nutrition', 'virology']):
                            subject_category = "biology"
                        elif any(chem_term in subject_lower for chem_term in ['chemistry']):
                            subject_category = "chemistry"
                        elif any(phys_term in subject_lower for phys_term in ['physics']):
                            subject_category = "physics"
                        elif any(math_term in subject_lower for math_term in ['mathematics', 'math']):
                            subject_category = "mathematics"
                        elif any(cs_term in subject_lower for cs_term in ['computer', 'machine_learning']):
                            subject_category = "computer_science"
                        elif any(phil_term in subject_lower for phil_term in ['philosophy', 'logic']):
                            subject_category = "philosophy"
                        elif subject_lower == 'astronomy':
                            subject_category = "astronomy"

                        # FIXED: Better formatting with proper structure
                        input_text = f"[MCQ] [SUBJECT:{subject_category}] Topic: {subject.replace('_', ' ').title()}\n\nGenerate a multiple choice question with 1 correct answer and 3 wrong options."

                        target_output = f"Question: {question}\n\nCorrect Answer: {correct_answer}\n\nWrong Option 1: {wrong_options[0]}\n\nWrong Option 2: {wrong_options[1]}\n\nWrong Option 3: {wrong_options[2] if len(wrong_options) > 2 else 'N/A'}"

                        records.append({
                            "text": input_text,
                            "ctext": target_output,
                            "answer": target_output,
                            "subject": subject_category,
                            "original_subject": subject,  # Keep track of original
                            "task": "mcq_generation",
                            "domain": "education"
                        })

            except Exception as e:
                print(f"Error loading MMLU subject {subject}: {e}")
                continue

        df = pd.DataFrame(records)
        print(f"Loaded {len(df)} MMLU examples")

        # Show distribution by subject
        if len(df) > 0:
            print("\nSubject distribution:")
            print(df['subject'].value_counts())
            print(f"\nOriginal subjects: {df['original_subject'].unique()}")

        return df

    except Exception as e:
        print(f"Error loading MMLU: {e}")
        return pd.DataFrame(columns=["text", "ctext", "answer", "subject", "task", "domain"])

def load_race_dataset(max_samples: int = None) -> pd.DataFrame:
    """
    IMPROVED: Better formatting and content handling for RACE dataset
    """
    print("Loading RACE for MCQ GENERATION (IMPROVED FORMAT)...")
    try:
        # Load both middle and high school datasets
        datasets = []
        levels_loaded = []

        for level in ['middle', 'high']:
            try:
                ds = load_dataset("race", level, split="train")
                datasets.extend(list(ds))
                levels_loaded.append(level)
                print(f"Loaded {len(ds)} examples from RACE-{level}")
            except Exception as e:
                print(f"Could not load RACE-{level}: {e}")
                continue

        print(f"Total examples available: {len(datasets)}")

        if max_samples and len(datasets) > max_samples:
            random.shuffle(datasets)
            datasets = datasets[:max_samples]

        records = []
        for example in tqdm(datasets, desc="Processing RACE"):
            article = example.get("article", "").strip()
            question = example.get("question", "").strip()
            options = example.get("options", [])
            answer = example.get("answer", "")

            if (article and question and options and answer and
                len(options) >= 4 and answer in ['A', 'B', 'C', 'D']):

                # Get correct answer text
                answer_idx = ord(answer) - ord('A')
                if answer_idx < len(options):
                    correct_answer = options[answer_idx].strip()
                    wrong_options = [options[i].strip() for i in range(len(options)) if i != answer_idx]

                    # IMPROVED: Better content extraction with smart truncation
                    content_sentences = article.split('.')
                    content = ""
                    word_count = 0

                    for sentence in content_sentences:
                        sentence = sentence.strip()
                        if not sentence:
                            continue
                        sentence_words = sentence.split()
                        if word_count + len(sentence_words) <= 150:  # Reduced from 200 for better readability
                            content += sentence + ". "
                            word_count += len(sentence_words)
                        else:
                            break

                    content = content.strip()
                    if not content.endswith('.'):
                        content += "."

                    # IMPROVED: Better subject categorization based on content
                    subject = "reading_comprehension"  # More specific than "general"

                    # Try to infer subject from content keywords
                    content_lower = content.lower() + " " + question.lower()
                    if any(sci_word in content_lower for sci_word in ['science', 'research', 'study', 'experiment', 'scientific']):
                        subject = "science"
                    elif any(hist_word in content_lower for hist_word in ['history', 'historical', 'century', 'ancient', 'war']):
                        subject = "history"
                    elif any(lit_word in content_lower for lit_word in ['literature', 'novel', 'author', 'poem', 'story']):
                        subject = "literature"
                    elif any(soc_word in content_lower for soc_word in ['society', 'social', 'culture', 'community']):
                        subject = "social_studies"

                    # IMPROVED: Better formatting with cleaner structure
                    input_text = f"[MCQ] [SUBJECT:{subject}] Reading Passage:\n\n{content}\n\nGenerate a multiple choice question with 1 correct answer and 3 wrong options based on this passage."

                    target_output = f"Question: {question}\n\nCorrect Answer: {correct_answer}\n\nWrong Option 1: {wrong_options[0]}\n\nWrong Option 2: {wrong_options[1]}\n\nWrong Option 3: {wrong_options[2] if len(wrong_options) > 2 else 'Additional option'}"

                    records.append({
                        "text": input_text,
                        "ctext": target_output,
                        "answer": target_output,
                        "subject": subject,
                        "task": "mcq_generation",
                        "domain": "education",
                        "content_length": len(content.split()),
                        "difficulty_level": "middle" if any("middle" in str(ex) for ex in [example]) else "high"
                    })

        df = pd.DataFrame(records)
        print(f"Loaded {len(df)} RACE examples")

        # Show distribution stats
        if len(df) > 0:
            print(f"\nSubject distribution:")
            print(df['subject'].value_counts())
            print(f"\nAverage content length: {df['content_length'].mean():.1f} words")
            print(f"Content length range: {df['content_length'].min()}-{df['content_length'].max()} words")

        return df

    except Exception as e:
        print(f"Error loading RACE: {e}")
        return pd.DataFrame(columns=["text", "ctext", "answer", "subject", "task", "domain"])

def load_sciq_dataset(max_samples: int = None) -> pd.DataFrame:
    """
    IMPROVED: Better subject detection and formatting for SciQ dataset
    """
    print("Loading SciQ for MCQ GENERATION (IMPROVED FORMAT)...")
    try:
        ds = load_dataset("sciq", split="train")
        print(f"Total SciQ examples available: {len(ds)}")

        if max_samples and len(ds) > max_samples:
            indices = list(range(len(ds)))
            random.shuffle(indices)
            indices = indices[:max_samples]
            ds = ds.select(indices)

        records = []
        for example in tqdm(ds, desc="Processing SciQ"):
            question = example.get("question", "").strip()
            correct_answer = example.get("correct_answer", "").strip()
            distractor1 = example.get("distractor1", "").strip()
            distractor2 = example.get("distractor2", "").strip()
            distractor3 = example.get("distractor3", "").strip()
            support = example.get("support", "").strip()

            if (question and correct_answer and distractor1 and distractor2 and distractor3):

                # IMPROVED: Better subject detection with more comprehensive keywords
                subject = "general_science"  # Default to general science

                # Combine question, answer, and support for better classification
                all_text = f"{question} {correct_answer} {support}".lower()

                # Biology keywords
                biology_terms = [
                    'cell', 'organism', 'biology', 'dna', 'protein', 'gene', 'bacteria',
                    'virus', 'tissue', 'organ', 'muscle', 'blood', 'heart', 'brain',
                    'ecosystem', 'species', 'evolution', 'photosynthesis', 'respiration',
                    'mitosis', 'meiosis', 'chromosome', 'enzyme', 'membrane', 'nucleus',
                    'animal', 'plant', 'fungi', 'reproduction', 'metabolism', 'hormone'
                ]

                # Chemistry keywords
                chemistry_terms = [
                    'chemical', 'molecule', 'reaction', 'atom', 'element', 'compound',
                    'ion', 'bond', 'acid', 'base', 'ph', 'electron', 'proton', 'neutron',
                    'periodic', 'carbon', 'oxygen', 'hydrogen', 'nitrogen', 'sodium',
                    'chlorine', 'calcium', 'oxide', 'solution', 'mixture', 'catalyst',
                    'formula', 'molecular', 'ionic', 'covalent', 'oxidation', 'reduction'
                ]

                # Physics keywords
                physics_terms = [
                    'force', 'energy', 'motion', 'physics', 'velocity', 'acceleration',
                    'mass', 'weight', 'gravity', 'friction', 'pressure', 'temperature',
                    'heat', 'light', 'wave', 'frequency', 'amplitude', 'sound', 'electric',
                    'magnetic', 'current', 'voltage', 'circuit', 'radiation', 'nuclear',
                    'particle', 'momentum', 'kinetic', 'potential', 'thermodynamics'
                ]

                # Earth Science keywords
                earth_science_terms = [
                    'earth', 'geology', 'rock', 'mineral', 'soil', 'atmosphere', 'weather',
                    'climate', 'ocean', 'water', 'earthquake', 'volcano', 'plate', 'fossil',
                    'sediment', 'erosion', 'mountain', 'continent', 'solar system', 'planet',
                    'moon', 'sun', 'star', 'galaxy', 'universe', 'asteroid', 'comet'
                ]

                # Count matches for each subject
                bio_count = sum(1 for term in biology_terms if term in all_text)
                chem_count = sum(1 for term in chemistry_terms if term in all_text)
                phys_count = sum(1 for term in physics_terms if term in all_text)
                earth_count = sum(1 for term in earth_science_terms if term in all_text)

                # Assign subject based on highest count (minimum threshold of 1)
                counts = {
                    'biology': bio_count,
                    'chemistry': chem_count,
                    'physics': phys_count,
                    'earth_science': earth_count
                }

                max_count = max(counts.values())
                if max_count > 0:
                    subject = max(counts, key=counts.get)
                else:
                    subject = "general_science"

                # IMPROVED: Better context handling - truncate long support text
                context = support if support else "General science content"
                if len(context) > 300:  # Truncate very long contexts
                    sentences = context.split('.')
                    context = ""
                    for sentence in sentences:
                        if len(context + sentence) < 250:
                            context += sentence + ". "
                        else:
                            break
                    context = context.strip()

                # IMPROVED: Better formatting
                input_text = f"[MCQ] [SUBJECT:{subject}] Science Context:\n\n{context}\n\nGenerate a multiple choice question with 1 correct answer and 3 wrong options."

                target_output = f"Question: {question}\n\nCorrect Answer: {correct_answer}\n\nWrong Option 1: {distractor1}\n\nWrong Option 2: {distractor2}\n\nWrong Option 3: {distractor3}"

                records.append({
                    "text": input_text,
                    "ctext": target_output,
                    "answer": target_output,
                    "subject": subject,
                    "task": "mcq_generation",
                    "domain": "education",
                    "context_length": len(context.split()),
                    "keyword_matches": max_count
                })

        df = pd.DataFrame(records)
        print(f"Loaded {len(df)} SciQ examples")

        # Show improved distribution stats
        if len(df) > 0:
            print(f"\nSubject distribution:")
            print(df['subject'].value_counts())
            print(f"\nAverage context length: {df['context_length'].mean():.1f} words")
            print(f"Average keyword matches: {df['keyword_matches'].mean():.1f}")

        return df

    except Exception as e:
        print(f"Error loading SciQ: {e}")
        return pd.DataFrame(columns=["text", "ctext", "answer", "subject", "task", "domain"])

def load_cnn_dailymail_summaries(max_samples: int = None) -> pd.DataFrame:
    """
    Load CNN/DailyMail dataset for summarization
    """
    print("Loading CNN/DailyMail dataset...")
    try:
        # Load without slicing
        ds = load_dataset("cnn_dailymail", "3.0.0", split="train")

        # Sample after loading
        if max_samples and len(ds) > max_samples:
            indices = list(range(len(ds)))
            random.shuffle(indices)
            indices = indices[:max_samples]
            ds = ds.select(indices)

        records = []
        for example in tqdm(ds, desc="Processing CNN/DailyMail"):
            article = example.get("article", "")
            highlights = example.get("highlights", "")

            if len(article.split()) >= 100 and len(highlights.split()) >= 20:
                records.append({
                    "text": article,
                    "ctext": highlights,
                    "subject": "news",
                    "task": "summarization",
                    "domain": "general"
                })

        df = pd.DataFrame(records)
        print(f"Loaded {len(df)} CNN/DailyMail examples")
        return df

    except Exception as e:
        print(f"Error loading CNN/DailyMail dataset: {e}")
        return pd.DataFrame(columns=["text", "ctext", "subject", "task", "domain"])

def load_code_alpaca_dataset(max_samples: int = None) -> pd.DataFrame:
    """
    Load Code Alpaca dataset for programming content
    """
    print("Loading Code Alpaca dataset...")
    try:
        # Try several alternative datasets for code
        try:
            # Try CodeAlpaca-20k
            ds = load_dataset("sahil2801/CodeAlpaca-20k", split="train")
        except:
            # Fallback to awesome-chatgpt-prompts
            ds = load_dataset("fka/awesome-chatgpt-prompts", split="train")

        # Sample after loading
        if max_samples and len(ds) > max_samples:
            indices = list(range(len(ds)))
            random.shuffle(indices)
            indices = indices[:max_samples]
            ds = ds.select(indices)

        records = []
        for example in tqdm(ds, desc="Processing Code Alpaca"):
            # Try to get fields based on dataset schema
            if "instruction" in example and "output" in example:
                instruction = example.get("instruction", "")
                output = example.get("output", "")
            elif "prompt" in example and "completion" in example:
                instruction = example.get("prompt", "")
                output = example.get("completion", "")
            else:
                continue

            if len(instruction.split()) >= 5 and len(output.split()) >= 20:
                # Extract programming language if mentioned
                lang_pattern = re.search(r"(Python|Java|C\+\+|JavaScript|Ruby|Go|Rust|PHP|SQL)", instruction)
                lang = lang_pattern.group(1) if lang_pattern else "general"

                records.append({
                    "text": instruction,
                    "ctext": output,
                    "subject": f"programming_{lang.lower()}",
                    "task": "summarization",  # Treating code generation as summarization
                    "domain": "computer_science"
                })

        df = pd.DataFrame(records)
        print(f"Loaded {len(df)} Code Alpaca examples")
        return df

    except Exception as e:
        print(f"Error loading Code Alpaca dataset: {e}")

        # Generate some simple code examples as fallback
        print("Generating synthetic code examples...")
        records = []

        # Programming topics
        topics = [
            "Sorting algorithm", "Binary search", "Linked list",
            "Hash table", "Graph traversal", "Dynamic programming"
        ]

        languages = ["Python", "Java", "JavaScript"]

        for i in range(200 if max_samples is None else min(200, max_samples)):
            topic = random.choice(topics)
            lang = random.choice(languages)

            instruction = f"Write a {topic} implementation in {lang}"
            output = f"Here's a {topic} implementation in {lang}:\n\n" + "# Code would go here\n" * 10

            records.append({
                "text": instruction,
                "ctext": output,
                "subject": f"programming_{lang.lower()}",
                "task": "summarization",
                "domain": "computer_science"
            })

        df = pd.DataFrame(records)
        print(f"Generated {len(df)} synthetic code examples")
        return df

def combine_educational_datasets(max_samples: int = 25000, shuffle: bool = True, seed: int = 42) -> pd.DataFrame:
    """
    FIXED: Consistent task handling and proper dataset combination
    """
    print("\n==== COMBINING DATASETS FOR QUIZ APP (FIXED) ====")

    random.seed(seed)

    # FIXED: Use consistent task names and remove broken loaders
    dataset_loaders = {
        load_openstax_textbooks: 0.15,           # task: "summarization"
        load_cnn_dailymail_summaries: 0.08,      # task: "summarization"
        load_code_alpaca_dataset: 0.07,          # task: "summarization"

        # MCQ GENERATION (100% for now since only these are working) - ALL use task: "mcq_generation"
        load_mmlu_dataset: 0.30,                 # FIXED function
        load_sciq_dataset: 0.20,                 # FIXED function
        load_race_dataset: 0.20,                 # FIXED function
    }

    # Calculate samples per dataset
    total_weight = sum(dataset_loaders.values())
    dataset_samples = {
        loader: int(weight * max_samples / total_weight)
        for loader, weight in dataset_loaders.items()
    }

    print("Dataset allocation:")
    for loader, samples in dataset_samples.items():
        print(f"  {loader.__name__}: {samples} samples")

    # Load data from each source
    dfs = []
    for loader, samples in dataset_samples.items():
        try:
            df = loader(max_samples=samples)
            if not df.empty:
                dfs.append(df)
                print(f"Added {len(df)} samples from {loader.__name__}")
        except Exception as e:
            print(f"Error loading {loader.__name__}: {e}")

    if not dfs:
        print("No datasets loaded!")
        return pd.DataFrame(columns=["text", "ctext", "answer", "subject", "task", "domain"])

    # Combine all datasets
    combined = pd.concat(dfs, ignore_index=True)

    # FIXED: Proper task filtering
    summ_df = combined[combined['task'] == 'summarization'].reset_index(drop=True)
    mcq_df = combined[combined['task'] == 'mcq_generation'].reset_index(drop=True)

    print(f"Found {len(summ_df)} summarization and {len(mcq_df)} MCQ examples")

    # Balance: 30% summarization, 70% MCQ (for now just use MCQ since that's what we have)
    total_size = min(len(combined), max_samples)
    target_summ = int(total_size * 0.30)
    target_mcq = total_size - target_summ

    # Sample to achieve balance
    if len(summ_df) > target_summ:
        summ_df = summ_df.sample(n=target_summ, random_state=seed)
    elif len(summ_df) < target_summ and len(summ_df) > 0:
        # Duplicate if needed
        ratio = target_summ / len(summ_df)
        repeats = int(ratio)
        remainder = ratio - repeats

        if repeats > 1:
            summ_df = pd.concat([summ_df] * repeats, ignore_index=True)
        if remainder > 0:
            extra = summ_df.sample(frac=remainder, random_state=seed)
            summ_df = pd.concat([summ_df, extra], ignore_index=True)

    if len(mcq_df) > target_mcq:
        mcq_df = mcq_df.sample(n=target_mcq, random_state=seed)
    elif len(mcq_df) < target_mcq and len(mcq_df) > 0:
        # Duplicate if needed
        ratio = target_mcq / len(mcq_df)
        repeats = int(ratio)
        remainder = ratio - repeats

        if repeats > 1:
            mcq_df = pd.concat([mcq_df] * repeats, ignore_index=True)
        if remainder > 0:
            extra = mcq_df.sample(frac=remainder, random_state=seed)
            mcq_df = pd.concat([mcq_df, extra], ignore_index=True)

    # Combine and shuffle
    balanced = pd.concat([summ_df, mcq_df], ignore_index=True)

    if shuffle:
        balanced = balanced.sample(frac=1, random_state=seed).reset_index(drop=True)

    print(f"\nFINAL DATASET: {len(balanced)} samples")
    print(f"Summarization: {len(balanced[balanced['task'] == 'summarization'])} ({len(balanced[balanced['task'] == 'summarization'])/len(balanced)*100:.1f}%)")
    print(f"MCQ Generation: {len(balanced[balanced['task'] == 'mcq_generation'])} ({len(balanced[balanced['task'] == 'mcq_generation'])/len(balanced)*100:.1f}%)")

    return balanced

"""Extracting, Cleaning, Chunking Text from Pdf File and Emebdding the Cleaned Text"""

class EducationalPDFProcessor:
    def __init__(
        self,
        embedding_model: str = "sentence-transformers/all-mpnet-base-v2",
        max_segment_words: int = 150,
        training_data: Optional[pd.DataFrame] = None
    ):
        self.embedder = SentenceTransformer(embedding_model)
        self.max_segment_words = max_segment_words
        self.training_data = training_data
        self.stopwords = set(stopwords.words('english'))
        self.subject_patterns = {
            "mathematics": r'\b(equation|theorem|algebra|calculus|geometry|math|formula|polynomial|function)\b',
            "science": r'\b(biology|chemistry|physics|scientific|experiment|hypothesis|molecule|atom|cell)\b',
            "literature": r'\b(novel|poem|author|literary|character|fiction|metaphor|narrator|plot)\b',
            "history": r'\b(century|historical|ancient|medieval|civilization|revolution|dynasty|empire|king|queen)\b',
            "computer_science": r'\b(algorithm|programming|software|hardware|code|database|computation|binary|variable)\b'
        }

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """
        Extract text from PDFs of any subject with better structure preservation
        """
        text = ""
        errors = []

        # Try multiple extraction methods (same core approach as original)
        try:
            pages = []
            with pdfplumber.open(pdf_path) as pdf:
                # Extract metadata first
                metadata = {}
                if hasattr(pdf, 'metadata') and pdf.metadata:
                    for key, value in pdf.metadata.items():
                        if value and isinstance(value, str):
                            metadata[key] = value

                # Extract document structure info
                toc = pdf.outline if hasattr(pdf, 'outline') else []

                # Extract text page by page with structure preservation
                for page_num, page in enumerate(pdf.pages, 1):
                    # Extract text with good spacing parameters
                    page_text = page.extract_text(x_tolerance=3, y_tolerance=3)

                    if page_text:
                        # Add page number reference
                        processed_text = f"--- Page {page_num} ---\n{page_text}"
                        pages.append(processed_text)

                    # Handle images if relevant to document understanding
                    # (this would require additional image processing libraries)

                # Combine metadata and text
                if metadata:
                    meta_text = "--- Document Metadata ---\n"
                    for key, value in metadata.items():
                        if key in ['Title', 'Author', 'Subject', 'Keywords'] and value:
                            meta_text += f"{key}: {value}\n"
                    if len(meta_text) > 30:  # Only add if we have meaningful metadata
                        pages.insert(0, meta_text)

                # Add table of contents if available
                if toc:
                    toc_text = "--- Table of Contents ---\n"
                    for item in toc:
                        if isinstance(item, dict) and 'title' in item:
                            toc_text += f"• {item['title']}\n"
                    if len(toc_text) > 30:  # Only add if we have meaningful TOC
                        pages.insert(1 if metadata else 0, toc_text)

            if pages:
                text = "\n\n".join(pages)
                print(f"Successfully extracted {len(pages)} pages with pdfplumber")
        except Exception as e:
            errors.append(f"Pdfplumber extraction failed: {str(e)}")

        # If primary method failed, try fallbacks (as in original code)

        # Post-processing specific to document type
        if text:
            # Detect document type/subject
            subject = self.identify_subject_area(text)

            # Apply appropriate structure preservation based on subject
            if subject in ["computer_science", "programming"]:
                text = self._preserve_code_structure(text)
            elif subject in ["mathematics", "physics"]:
                text = self._preserve_formula_structure(text)
            elif subject in ["literature", "humanities"]:
                text = self._preserve_narrative_structure(text)
            else:
                text = self._preserve_general_structure(text)

        return text

    def _fix_encoding_issues(self, text: str) -> str:
        """Fix common encoding issues in extracted PDF text"""
        # Replace common encoding artifacts
        replacements = {
            '\uf0b7': '•',  # Common bullet point replacement
            '\u2022': '•',  # Another bullet point
            '\u00a0': ' ',  # Non-breaking space
            '\u2019': "'",  # Right single quotation mark
            '\u2018': "'",  # Left single quotation mark
            '\u201c': '"',  # Left double quotation mark
            '\u201d': '"',  # Right double quotation mark
            '\u2013': '-',  # En dash
            '\u2014': '--', # Em dash
            '\ufb01': 'fi', # fi ligature
            '\ufb02': 'fl', # fl ligature
        }

        for old, new in replacements.items():
            text = text.replace(old, new)

        # Remove control characters
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in ('\n', '\t'))

        return text

    def _fix_ligatures(self, text: str) -> str:
        """Fix common ligature issues in PDFs"""
        ligatures = {
            'ﬀ': 'ff', 'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬃ': 'ffi', 'ﬄ': 'ffl',
            'Ꜳ': 'AA', 'ꜳ': 'aa', 'Æ': 'AE', 'æ': 'ae', 'Ꜵ': 'AO',
            'ꜵ': 'ao', 'Ꜷ': 'AV', 'ꜷ': 'av', 'Ꜹ': 'AV', 'ꜹ': 'av',
            'Ꜻ': 'AY', 'ꜻ': 'ay', 'Œ': 'OE', 'œ': 'oe', 'Ꝏ': 'OO',
            'ꝏ': 'oo', 'ẞ': 'ss', 'ﬆ': 'st'
        }

        for ligature, replacement in ligatures.items():
            text = text.replace(ligature, replacement)

        return text

    def _fix_line_breaks(self, text: str) -> str:
        """Fix improper line breaks and hyphenation from PDFs"""
        # Fix cases where words are broken across lines with hyphens
        text = re.sub(r'(\w+)-\n(\w+)', lambda m: m.group(1) + m.group(2), text)

        # Fix unnecessary line breaks in the middle of sentences
        text = re.sub(r'([a-z,;:])\n([a-z])', r'\1 \2', text)

        # Normalize multiple newlines to just two
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text

    def _fix_column_merging(self, text: str) -> str:
        """
        Attempt to fix issues when PDF columns get merged incorrectly
        This is a heuristic approach and may not work for all documents
        """
        lines = text.split('\n')
        fixed_lines = []

        # Look for patterns that suggest incorrectly merged columns
        for line in lines:
            # Check for repeated spacing patterns that might indicate columns
            spaces = [match.start() for match in re.finditer(r' {3,}', line)]
            if len(spaces) >= 2:
                # Check if spaces are at regular intervals, suggesting columns
                intervals = [spaces[i+1] - spaces[i] for i in range(len(spaces)-1)]
                if max(intervals) - min(intervals) < 5 and len(intervals) >= 2:
                    # Likely column structure, insert newlines
                    parts = []
                    last_pos = 0
                    for pos in spaces:
                        parts.append(line[last_pos:pos].strip())
                        last_pos = pos
                    parts.append(line[last_pos:].strip())
                    fixed_lines.extend([p for p in parts if p])
                    continue

            fixed_lines.append(line)

        return '\n'.join(fixed_lines)

    def _is_likely_garbage(self, text: str) -> bool:
        """
        Check if extracted text is likely garbage (encrypted PDF or image-only)
        """
        # Check for high percentage of non-alphanumeric characters
        total_chars = len(text)
        if total_chars == 0:
            return True

        alpha_chars = sum(c.isalpha() or c.isspace() for c in text)
        alpha_ratio = alpha_chars / total_chars

        # Check for long strings of random-looking characters
        has_long_gibberish = bool(re.search(r'[^\s\w.,;:(){}\[\]]{20,}', text))

        # Check for complete lack of spaces or common words
        has_spaces = ' ' in text
        has_common_words = any(word in text.lower() for word in ['the', 'and', 'is', 'in', 'to', 'of'])

        # If text has very few alphabetic characters or contains long gibberish strings
        if alpha_ratio < 0.5 or has_long_gibberish:
            return True

        # If text has no spaces or common words
        if not has_spaces or not has_common_words:
            return True

        return False

    def clean_pdf_text(self, text: str) -> str:
        """
        Enhanced PDF text cleaning with better artifact removal
        """
        # Apply base cleaning first
        text = self._fix_encoding_issues(text)
        text = self._fix_ligatures(text)
        text = self._fix_line_breaks(text)

        # Enhanced cleaning steps

        # Remove common PDF artifacts
        artifacts_patterns = [
            r'\b\d+\s+of\s+\d+\b',  # Page numbers "1 of 10"
            r'©\s*\d{4}.*?(?:\n|$)',  # Copyright notices
            r'All rights reserved.*?(?:\n|$)',  # Rights statements
            r'www\.\S+\.\w+',  # Web addresses
            r'\b[A-Z]{2,}\s+\d+\b',  # Course codes like "CS 101"
            r'^\s*\d+\.\s*$',  # Standalone numbers
            r'^\s*[A-Z]\.\s*$',  # Standalone letters
        ]

        for pattern in artifacts_patterns:
            text = re.sub(pattern, '', text, flags=re.MULTILINE | re.IGNORECASE)

        # Enhanced paragraph filtering
        paragraphs = text.split('\n\n')
        quality_paragraphs = []

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            words = para.split()

            # Enhanced quality criteria
            if (len(words) >= 15 and  # Minimum length
                len(para) >= 80 and   # Minimum character count
                # Must contain educational content indicators
                any(indicator in para.lower() for indicator in [
                    'is', 'are', 'means', 'refers', 'defined', 'called', 'known',
                    'process', 'method', 'approach', 'technique', 'principle',
                    'function', 'purpose', 'allows', 'enables', 'provides',
                    'because', 'since', 'therefore', 'however', 'although',
                    'important', 'significant', 'key', 'main', 'primary'
                ]) and
                # Should not be mostly technical jargon without context
                len([w for w in words if len(w) > 10]) / len(words) < 0.3 and
                # Should not be mostly numbers
                len([w for w in words if re.match(r'^[\d\.\-/]+$', w)]) / len(words) < 0.2):

                quality_paragraphs.append(para)

        return '\n\n'.join(quality_paragraphs)

    def _is_quality_segment(self, segment: str) -> bool:
        """
        Enhanced quality check for text segments
        """
        words = segment.split()

        # Basic length requirements
        if len(words) < 20 or len(segment) < 100:
            return False

        # Should not be mostly numbers or technical symbols
        numeric_ratio = len([w for w in words if re.match(r'^[\d\.\-/]+$', w)]) / len(words)
        if numeric_ratio > 0.4:
            return False

        # Must contain educational content indicators
        educational_indicators = [
            'is', 'are', 'means', 'refers to', 'defined as', 'called', 'known as',
            'process', 'method', 'approach', 'technique', 'principle', 'concept',
            'function', 'purpose', 'allows', 'enables', 'provides', 'creates',
            'because', 'since', 'therefore', 'however', 'although', 'while',
            'important', 'significant', 'key', 'main', 'primary', 'essential'
        ]

        has_educational_content = any(
            indicator in segment.lower() for indicator in educational_indicators
        )

        if not has_educational_content:
            return False

        # Should have proper sentence structure
        sentences = sent_tokenize(segment)
        if len(sentences) < 2:  # Need multiple sentences for context
            return False

        # Check for question artifacts (should not be mostly questions)
        question_sentences = [s for s in sentences if s.strip().endswith('?')]
        if len(question_sentences) / len(sentences) > 0.5:
            return False

        return True

    def semantic_segment_text(self, text: str) -> List[str]:
        """
        Enhanced semantic segmentation with better content validation
        """
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        segments = []

        for para in paragraphs:
            words = len(para.split())

            if words <= self.max_segment_words:
                if self._is_quality_segment(para):
                    segments.append(para)
            else:
                # Break down large paragraphs more intelligently
                sentences = sent_tokenize(para)
                current_segment = []
                current_word_count = 0

                for sentence in sentences:
                    sentence_words = len(sentence.split())

                    if current_word_count + sentence_words <= self.max_segment_words:
                        current_segment.append(sentence)
                        current_word_count += sentence_words
                    else:
                        # Finalize current segment
                        if current_segment:
                            segment_text = ' '.join(current_segment)
                            if self._is_quality_segment(segment_text):
                                segments.append(segment_text)

                        # Start new segment
                        current_segment = [sentence]
                        current_word_count = sentence_words

                # Add final segment
                if current_segment:
                    segment_text = ' '.join(current_segment)
                    if self._is_quality_segment(segment_text):
                        segments.append(segment_text)

        return segments

    def identify_subject_area(self, text: str) -> str:
        text_lower = text.lower()
        scores = {subject: len(re.findall(pattern, text_lower))
                  for subject, pattern in self.subject_patterns.items()}
        detected = max(scores, key=scores.get)
        return detected if scores[detected] > 0 else "general"

    def extract_key_entities(self, text: str, n: int = 15) -> List[str]:
        words = word_tokenize(text.lower())
        content = [w for w in words if w.isalpha() and w not in self.stopwords]
        freq = {}
        for w in content:
            freq[w] = freq.get(w, 0) + 1
        return [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:n]]

    def generate_educational_questions(self, text: str, subject: str) -> List[str]:
        """Generate relevant educational questions based on subject area"""
        # Subject-specific question templates
        question_templates = {
            "mathematics": [
                "What is the main theorem or concept presented in this text?",
                "Explain the steps to solve the mathematical problem discussed.",
                "What are the key formulas introduced in this passage?",
                "How would you apply this mathematical concept in a real-world scenario?",
                "Describe the relationship between the different mathematical entities mentioned."
            ],
            "science": [
                "What is the key scientific principle explained in this text?",
                "Describe the experimental procedure mentioned in the passage.",
                "How does this scientific concept relate to everyday phenomena?",
                "What evidence supports the scientific claims made in this text?",
                "Compare and contrast the different scientific processes described."
            ],
            "literature": [
                "Who are the main characters discussed in this text?",
                "What literary devices are employed in this passage?",
                "How does the author develop the themes mentioned in this text?",
                "Analyze the narrative structure of the described work.",
                "What is the historical or cultural context of this literary piece?"
            ],
            "history": [
                "What historical period or event is described in this passage?",
                "Who were the key historical figures mentioned and what was their significance?",
                "How did this historical event impact subsequent developments?",
                "What primary sources are referenced in this historical analysis?",
                "Compare this historical event with a similar one from a different time period."
            ],
            "computer_science": [
                "What computational concept is explained in this text?",
                "Explain the algorithm or process described in the passage.",
                "How would you implement the described functionality in code?",
                "What are the time and space complexity considerations for this approach?",
                "How does this concept connect to other areas of computer science?"
            ],
            "general": [
                "What are the main concepts presented in this text?",
                "Summarize the key points from this passage.",
                "How would you explain this concept to someone new to the subject?",
                "What supporting evidence is provided for the main claims?",
                "How does this information connect to related fields of study?"
            ]
        }

        # Select questions based on detected subject
        selected_templates = question_templates.get(subject, question_templates["general"])

        return selected_templates

    def embed_segments(self, segments: List[str], batch_size: int = 32) -> np.ndarray:
        embs = []
        for i in range(0, len(segments), batch_size):
            batch = segments[i: i+batch_size]
            embs.append(self.embedder.encode(batch, convert_to_numpy=True, normalize_embeddings=True))
        return np.vstack(embs)

    def build_faiss_index(self, embeddings: np.ndarray) -> faiss.IndexFlatIP:
        d = embeddings.shape[1]
        index = faiss.IndexFlatIP(d)
        index.add(embeddings)
        return index

    def retrieve_topk_segments(self,
        query: str,
        segments: List[str],
        index: faiss.IndexFlatIP,
        k: int = 5
    ) -> List[Tuple[str, float]]:
        q_emb = self.embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        scores, idxs = index.search(q_emb, k)
        return [(segments[int(idxs[0][i])], float(scores[0][i])) for i in range(k)]

    def retrieve_central_segments(
        self,
        segments: List[str],
        embeddings: np.ndarray,
        index: faiss.IndexFlatIP,
        k: int = 5
    ) -> List[Tuple[str, float]]:
        centroid = embeddings.mean(axis=0, keepdims=True)
        centroid /= np.linalg.norm(centroid, axis=1, keepdims=True)
        scores, idxs = index.search(centroid, k)
        segment_scores = [(segments[int(idxs[0][i])], float(scores[0][i])) for i in range(k)]
        filtered = []
        for seg, score in segment_scores:
            if re.search(r'^\s*\d+\.\d*\s+[A-Z].*$', seg, re.MULTILINE): continue
            if len(re.findall(r'\d+\.\d+', seg)) > 3: continue
            filtered.append((seg, score))
        return filtered[:k]

    def select_diverse_segments(self, segments: List[str], max_segments: int = 20) -> List[str]:
        if len(segments) <= max_segments:
            return segments
        step = len(segments) / max_segments
        return [segments[int(i * step)] for i in range(max_segments)]

    def _is_academic_paper(self, text: str) -> bool:
        academic_indicators = [
            r'\babstract\b', r'\bintroduction\b', r'\bmethodology\b', r'\bliterature review\b',
            r'\bconclusion\b', r'\breferences\b', r'\bcitation[s]?\b', r'\bet al\b',
            r'\bp[<\s]?[=<>]\s?0\.0[0-9]\b'
        ]
        indicator_count = sum(bool(re.search(p, text, re.IGNORECASE)) for p in academic_indicators)
        citation_patterns = [r'\(\s*[A-Z][a-z]+(?: et al\.)?,\s*\d{4}\)', r'\[\s*\d+\s*\]']
        citation_count = sum(len(re.findall(p, text)) for p in citation_patterns)
        return (indicator_count >= 3) or (citation_count >= 5)

    def _extract_abstract(self, segments: List[str]) -> List[str]:
        for i, seg in enumerate(segments[:10]):
            if re.search(r'\babstract\b', seg, re.IGNORECASE):
                res = [seg]
                if i + 1 < len(segments):
                    res.append(segments[i+1])
                return res
        return []

    def _identify_section_segments(self, segments: List[str], keywords: List[str]) -> List[str]:
        out, in_sec = [], False
        for seg in segments:
            if any(re.search(rf'\b{k}\b', seg, re.IGNORECASE) for k in keywords):
                in_sec = True
                out.append(seg)
            elif in_sec:
                if re.search(r'^[0-9\.]+\s+[A-Z]', seg) or re.search(r'^[A-Z][a-z]+\s*$', seg):
                    break
                out.append(seg)
        return out

    def process_educational_pdf(self, pdf_path: str) -> Dict:
        raw = self.extract_text_from_pdf(pdf_path)
        cleaned = self.clean_pdf_text(raw)
        is_academic = self._is_academic_paper(cleaned)
        segments = self.semantic_segment_text(cleaned)

        if is_academic:
            abstract = self._extract_abstract(segments)
            methods_seg = self._identify_section_segments(segments, ['method', 'methodology', 'approach', 'experiment'])
            results_seg = self._identify_section_segments(segments, ['result', 'finding', 'outcome'])
            disc_seg    = self._identify_section_segments(segments, ['discussion', 'conclusion', 'implication'])
            structured = []
            if abstract:
                structured.extend(abstract)
            for secs in (methods_seg, results_seg, disc_seg):
                if secs:
                    embs = self.embed_segments(secs)
                    idx  = self.build_faiss_index(embs)
                    central = self.retrieve_central_segments(secs, embs, idx, k=min(3, len(secs)))
                    structured.extend([s for s,_ in central])
            if structured:
                segments = structured

        embs = self.embed_segments(segments)
        index = self.build_faiss_index(embs)
        subject = self.identify_subject_area(cleaned)
        suggested_questions = self.generate_educational_questions(cleaned, subject)
        central_segs = self.retrieve_central_segments(segments, embs, index, k=min(5, len(segments)))
        central_texts = [s for s,_ in central_segs]
        key_entities = self.extract_key_entities(cleaned)

        return {
            'text': cleaned,
            'segments': segments,
            'embeddings': embs,
            'index': index,
            'subject': subject,
            'suggested_questions': suggested_questions,
            'central_texts': central_texts,
            'key_entities': key_entities,
            'is_academic': is_academic
        }

    def _preserve_code_structure(self, text: str) -> str:
        """
        Preserve structure specific to code and programming text
        Focus on indentation, code blocks, and proper spacing
        """
        # Identify and protect code blocks (text between triple backticks or indented blocks)
        text = re.sub(r'```(?:python|java|cpp|c\+\+|javascript|js)?\n(.*?)```',
                      lambda m: '##CODE_BLOCK##' + m.group(1).replace('\n', '##NEWLINE##') + '##END_CODE_BLOCK##',
                      text, flags=re.DOTALL)

        # Protect indented code blocks (4+ spaces or tab at start of line)
        lines = text.split('\n')
        in_indent_block = False
        for i, line in enumerate(lines):
            if re.match(r'^\s{4,}|\t', line) and line.strip():
                in_indent_block = True
                lines[i] = '##CODE_LINE##' + line + '##END_CODE_LINE##'
            elif in_indent_block and not line.strip():
                # Preserve empty lines in code blocks
                lines[i] = '##CODE_LINE####END_CODE_LINE##'
            elif in_indent_block and not re.match(r'^\s{4,}|\t', line) and line.strip():
                in_indent_block = False

        text = '\n'.join(lines)

        # Fix common code structure issues
        # Preserve operators with proper spacing
        text = re.sub(r'(\w)([+\-*/=<>!&|])(\w)', r'\1 \2 \3', text)

        # Restore code blocks
        text = text.replace('##NEWLINE##', '\n')
        text = re.sub(r'##CODE_BLOCK##(.*?)##END_CODE_BLOCK##',
                      lambda m: f'```\n{m.group(1)}\n```',
                      text, flags=re.DOTALL)
        text = re.sub(r'##CODE_LINE##(.*?)##END_CODE_LINE##', r'\1', text)

        # Handle special case of variable definitions and function calls
        text = re.sub(r'(\w+)\s*\(\s*([^)]*)\s*\)', r'\1(\2)', text)

        return text

    def _preserve_formula_structure(self, text: str) -> str:
        """
        Preserve mathematical formulas and equation structures
        Focus on equations, mathematical symbols, and proper formatting
        """
        # Identify and protect inline math expressions (e.g., $x^2 + y^2 = z^2$)
        text = re.sub(r'\$([^$]+)\$',
                      lambda m: '##MATH_INLINE##' + m.group(1).replace('\n', ' ') + '##END_MATH_INLINE##',
                      text)

        # Identify and protect block math expressions (e.g., $$x^2 + y^2 = z^2$$)
        text = re.sub(r'\$\$([^$]+)\$\$',
                    lambda m: '##MATH_BLOCK##' + m.group(1).replace('\n', '##NEWLINE##') + '##END_MATH_BLOCK##',
                    text, flags=re.DOTALL)

        # Fix spacing in mathematical expressions (don't add spaces between operators in formulas)
        def fix_math_spacing(match):
            content = match.group(1)
            # Don't adjust spacing in math content
            return '##MATH_INLINE##' + content + '##END_MATH_INLINE##'

        text = re.sub(r'##MATH_INLINE##([^#]+)##END_MATH_INLINE##', fix_math_spacing, text)

        # Preserve proper formatting for common math patterns
        text = re.sub(r'(\d+)\s*\/\s*(\d+)', r'\1/\2', text)  # Fractions like 1/2
        text = re.sub(r'(\w+)\s*\^\s*(\w+)', r'\1^\2', text)  # Exponents like x^2

        # Restore math expressions
        text = text.replace('##NEWLINE##', '\n')
        text = re.sub(r'##MATH_INLINE##([^#]+)##END_MATH_INLINE##', r'$\1$', text)
        text = re.sub(r'##MATH_BLOCK##([^#]+)##END_MATH_BLOCK##', r'$$\1$$', text, flags=re.DOTALL)

        return text

    def _preserve_narrative_structure(self, text: str) -> str:
        """
        Preserve narrative flow and structure for literature and humanities texts
        Focus on paragraphs, quotes, and dialogue
        """
        # Preserve quotations and dialogue
        text = re.sub(r'"([^"]+)"', r'"\1"', text)

        # Ensure proper paragraph breaks
        text = re.sub(r'([.!?])\s*\n\s*([A-Z])', r'\1\n\n\2', text)

        # Preserve poetic structure and indentation in literary works
        lines = text.split('\n')
        in_poetry = False
        poetry_lines = []

        for i, line in enumerate(lines):
            # Detect potential poetry by looking for short lines with similar indentation
            if len(line.strip()) > 0 and len(line.strip()) < 40 and line.strip()[0].isupper():
                next_line_idx = i + 1
                if next_line_idx < len(lines) and len(lines[next_line_idx].strip()) > 0:
                    next_line = lines[next_line_idx].strip()
                    if len(next_line) < 40 and next_line[0].isupper():
                        # This looks like poetry
                        in_poetry = True
                        poetry_lines.append(i)
                        poetry_lines.append(next_line_idx)

        # Preserve paragraph structure for normal text
        if not in_poetry:
            text = re.sub(r'([a-z,;:])\n([a-z])', r'\1 \2', text)  # Join broken sentences
        else:
            # Keep poetry line breaks
            for i in poetry_lines:
                if i < len(lines):
                    lines[i] = '##POETRY##' + lines[i] + '##END_POETRY##'
            text = '\n'.join(lines)
            text = re.sub(r'##POETRY##([^#]+)##END_POETRY##', r'\1', text)

        return text

    def _preserve_general_structure(self, text: str) -> str:
        """
        General structure preservation for documents that don't fit specific categories
        """
        # Fix hyphenation for line breaks
        text = re.sub(r'(\w+)-\n(\w+)', r'\1\2', text)

        # Handle bullet points and lists
        text = re.sub(r'•\s*', '* ', text)  # Standardize bullet points

        # Preserve paragraph structure
        text = re.sub(r'([a-z,;:"\')])[\n\s]+([a-z])', r'\1 \2', text)  # Join paragraphs
        text = re.sub(r'([.!?]")[\n\s]+([A-Z])', r'\1\n\n\2', text)  # Keep paragraph breaks

        # Make sure headers are properly separated
        text = re.sub(r'([a-z.!?])\n+([A-Z][A-Z\s]+:?)', r'\1\n\n\2', text)

        # Normalize multiple blank lines to just two
        text = re.sub(r'\n{3,}', '\n\n', text)

        return text

"""Creating the Dataset"""

TASK_SUMMARIZE = "summarize"
TASK_QA_MCQ = "qa_mcq"

class MultiTaskEducationalDataset(Dataset):
    """
    Combined dataset for both summarization and MCQ generation tasks
    with domain-specific formatting and task switching
    """
    def __init__(self,
                 source_texts,
                 summaries=None,
                 contexts=None,
                 questions=None,
                 answers=None,
                 subjects=None,
                 tokenizer=None,
                 max_source_len=512,
                 max_target_len=256,  # Increased for MCQ outputs
                 task_distribution=None):
        """
        Initialize with lists of content for all supported tasks

        Args:
            source_texts: List of document texts for summarization
            summaries: List of summaries (can be None if not using summarization)
            contexts: List of context passages for QA task (actually input prompts from datasets)
            questions: List of questions (can be None if not using pre-made QA pairs)
            answers: List of answers (actually target MCQ format from datasets)
            subjects: List of subject areas for each example
            tokenizer: Tokenizer to use for encoding
            max_source_len: Maximum source text length
            max_target_len: Maximum target text length
            task_distribution: Dict with task name keys and probability values
        """
        # Validate and store task mix
        self.task_distribution = task_distribution or {
            TASK_SUMMARIZE: 0.5,
            TASK_QA_MCQ: 0.5
        }
        # Normalize probabilities
        total_prob = sum(self.task_distribution.values())
        for task in self.task_distribution:
            self.task_distribution[task] /= total_prob

        # Store base content and parameters
        self.tokenizer = tokenizer
        self.max_source_len = max_source_len
        self.max_target_len = max_target_len

        # Store content for tasks
        self.source_texts = list(source_texts) if source_texts else []
        self.summaries = list(summaries) if summaries else [None] * len(self.source_texts)
        self.contexts = list(contexts) if contexts else []
        self.questions = list(questions) if questions else [None] * len(self.contexts)
        self.answers = list(answers) if answers else [None] * len(self.contexts)
        self.subjects = list(subjects) if subjects else ["general"] * max(len(self.source_texts), len(self.contexts))

        # Create full dataset by mixing tasks
        self.inputs = []
        self.targets = []
        self.task_types = []
        self.example_sources = []

        # Process examples into input-target pairs with task prefixes
        self.__preprocess()

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        labels = self.targets[idx].clone()
        labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': self.inputs[idx]["input_ids"].squeeze(),
            'attention_mask': self.inputs[idx]["attention_mask"].squeeze(),
            'labels': labels.squeeze(),
        }

    def __preprocess(self):
        """
        Process all available examples for all supported tasks
        """
        # Process summarization examples
        if self.source_texts:
            for idx, (text, summary) in enumerate(zip(self.source_texts, self.summaries)):
                if summary:
                    self._add_summarization_example(text, summary, idx)

        # Process MCQ examples
        if self.contexts:
            for idx, (input_prompt, question, target_mcq, subject) in enumerate(zip(
                    self.contexts, self.questions, self.answers, self.subjects[:len(self.contexts)])):

                # Since datasets provide complete input-output pairs:
                # input_prompt (from 'text' column): "[MCQ] [SUBJECT:biology] Context: ..."
                # target_mcq (from 'ctext' column): "Question: ...\nCorrect Answer: ...\nWrong Option 1: ..."
                if target_mcq is not None:
                    self._add_complete_mcq_example(input_prompt, target_mcq, subject, idx)

    def _add_complete_mcq_example(self, input_prompt, target_mcq, subject, idx):
        """
        Add complete MCQ example where both input prompt and target are provided from datasets
        """
        # Clean inputs
        cleaned_input = self._clean_text(input_prompt)
        cleaned_target = self._normalize_text(target_mcq)

        # Validate example quality
        if (len(cleaned_input.split()) >= 10 and  # Minimum input length
            len(cleaned_target.split()) >= 15 and  # Minimum target length
            "Question:" in cleaned_target and       # Must contain question
            "Correct Answer:" in cleaned_target):   # Must contain correct answer

            source_encoding = self._tokenize_source(cleaned_input)
            target_encoding = self._tokenize_target(cleaned_target)

            self.inputs.append(source_encoding)
            self.targets.append(target_encoding["input_ids"].squeeze())
            self.task_types.append(TASK_QA_MCQ)
            self.example_sources.append(f"complete_mcq_{idx}")

    def _add_summarization_example(self, text, summary, idx):
        """Enhanced summarization example processing"""
        cleaned_text = self._clean_text(text)
        normalized_summary = self._normalize_summary(summary)

        # Enhanced validation
        if (len(cleaned_text.split()) >= 25 and
            len(normalized_summary.split()) >= 8 and
            not normalized_summary.strip().endswith('?')):  # No questions as summaries

            prompt = f"[SUMMARIZE] {cleaned_text}"

            source_encoding = self._tokenize_source(prompt)
            target_encoding = self._tokenize_target(normalized_summary)

            self.inputs.append(source_encoding)
            self.targets.append(target_encoding["input_ids"].squeeze())
            self.task_types.append(TASK_SUMMARIZE)
            self.example_sources.append(f"summarize_{idx}")

    def _normalize_summary(self, summary):
        """Enhanced summary normalization"""
        summary = str(summary).strip()

        # Remove question artifacts
        if summary.strip().endswith('?'):
            sentences = sent_tokenize(summary)
            non_question_sentences = [s for s in sentences if not s.strip().endswith('?')]
            if non_question_sentences:
                summary = ' '.join(non_question_sentences)

        # Ensure proper capitalization and punctuation
        if summary and summary[0].islower():
            summary = summary[0].upper() + summary[1:]

        if summary and not summary.endswith(('.', '!', '?')):
            summary += '.'

        return summary

    def _tokenize_source(self, text):
        """Tokenize source text with proper handling"""
        return self.tokenizer(
            text,
            max_length=self.max_source_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

    def _tokenize_target(self, text):
        """Tokenize target text with proper handling"""
        return self.tokenizer(
            text,
            max_length=self.max_target_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

    def _clean_text(self, text):
        """Enhanced text cleaning"""
        text = str(text).strip()

        # Remove artifacts
        text = re.sub(r'\[\/?[^\]]*\]', '', text)  # Remove bracketed content
        text = re.sub(r'https?://\S+', '[URL]', text)  # Replace URLs
        text = re.sub(r'[\w\.-]+@[\w\.-]+', '[EMAIL]', text)  # Replace emails

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)

        return text

    def _normalize_text(self, text):
        """Normalize target text"""
        text = str(text).strip()

        # Clean up common artifacts in MCQ targets
        text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
        text = re.sub(r'\n+', '\n', text)  # Normalize newlines

        return text

"""Creating the Model"""

class MultiTaskEducationalModel:
    """
    Combined model class for educational content generation
    - Can generate summaries
    - Can generate QA pairs (open-ended)
    - Can generate multiple-choice questions with distractors
    """
    def __init__(
        self,
        model_name: str = "google/flan-t5-base",
        embedding_model: str = "sentence-transformers/all-mpnet-base-v2",
        device: str   = None
    ):
        # device & main model
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model     = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)

        # ←―― NEW: load your embedder here
        self.embedder = SentenceTransformer(embedding_model)

        # rest stays the same
        self.wordnet_pos_map = {
            'n': wordnet.NOUN, 'v': wordnet.VERB,
            'a': wordnet.ADJ,  'r': wordnet.ADV
        }
        self.nlp = spacy.load("en_core_web_sm")

        print(f"Model loaded on {self.device} with embedder `{embedding_model}`")


    def _enhanced_pdf_text_cleaning(self, text, subject=None):
        """
        UNIVERSAL: Systematic text cleaning that prevents common PDF extraction issues
        """
        # Step 1: Universal word boundary detection and separation
        # This handles ALL merged words systematically

        # 1.1: Insert spaces before capital letters that follow lowercase (camelCase fixing)
        text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)

        # 1.2: Insert spaces after punctuation that touches letters
        text = re.sub(r'([.!?,:;])([A-Za-z])', r'\1 \2', text)

        # 1.3: Separate numbers from letters
        text = re.sub(r'([a-z])(\d)', r'\1 \2', text)
        text = re.sub(r'(\d)([A-Za-z])', r'\1 \2', text)

        # Step 2: Universal long word detection and splitting
        # This automatically handles words that are obviously merged

        def smart_word_split(match):
            word = match.group(0)
            # If a word is longer than 20 characters and all lowercase, it's likely merged
            if len(word) > 20 and word.islower():
                # Look for common English word patterns and split
                # This uses linguistic patterns rather than specific words
                result = word

                # Split on common prefixes/suffixes
                for pattern in ['therefore', 'validity', 'assessment', 'limited', 'expected', 'level', 'market', 'cannot']:
                    if pattern in result and len(result) > len(pattern) + 3:
                        # Find the pattern and split after it
                        pos = result.find(pattern)
                        if pos >= 0 and pos + len(pattern) < len(result):
                            before = result[:pos + len(pattern)]
                            after = result[pos + len(pattern):]
                            if len(after) > 2:  # Only split if remainder is meaningful
                                result = before + ' ' + after
                                break
                return result
            return word

        # Apply smart splitting to long lowercase sequences
        text = re.sub(r'\b[a-z]{15,}\b', smart_word_split, text)

        # Step 3: Universal sentence beginning normalization
        # Handle incomplete sentence starters systematically

        # 3.1: Fix "Is empty/not empty" patterns
        text = re.sub(r'^Is\s+(empty|not\s+empty),', r'If it is \1,', text)

        # 3.2: Convert "Word: Description" to "Word is description"
        text = re.sub(r'^([A-Z][a-z]+):\s*([A-Z])', r'\1 is \2', text)
        text = re.sub(r'^([A-Z][a-z]+):\s*([a-z])', r'\1 is \2', text)

        # Step 4: Universal technical term spacing fixes
        # Handle programming and mathematical notation

        # 4.1: Fix method names with incorrect spacing
        text = re.sub(r'\bis\s+Present\(\)', 'isPresent()', text)
        text = re.sub(r'\bget\s+\(\)', 'get()', text)

        # 4.2: Fix exception names
        text = re.sub(r'\b([A-Z][a-z]+)\s+([A-Z][a-z]+)\s+Exception\b', r'\1\2Exception', text)

        # 4.3: Fix class names with spaces
        text = re.sub(r'\b([A-Z][a-z]+)\s+([A-Z][a-z]+)\s+([A-Z][a-z]+)\b',
                      lambda m: m.group(1) + m.group(2) + m.group(3)
                      if all(word[0].isupper() for word in [m.group(1), m.group(2), m.group(3)])
                      else m.group(0), text)

        # Step 5: Mathematical expression handling
        if subject != 'mathematics':
            # Remove mathematical symbols that become garbage in non-math contexts
            text = re.sub(r'[∈∂∀∃∑∏∫∪∩⊆⊇⊂⊃≤≥≠≈∞±×÷√∇∆◦•]+', ' ', text)
            # Remove isolated mathematical expressions
            text = re.sub(r'\b[A-Z]\s*=\s*[A-Z]\s*[+\-*/]\s*[A-Z]\b', ' ', text)

        # Step 6: Universal cleanup (same as before)
        text = re.sub(r'\b[cid:\d+]+\b', ' ', text)
        text = re.sub(r'\([cid:\d+]+\)', ' ', text)
        text = re.sub(r'[{}]+', ' ', text)
        text = re.sub(r'^\d+\s*/\s*\d+.*?$', '', text, flags=re.MULTILINE)
        text = re.sub(r'^Page\s+\d+.*?$', '', text, flags=re.MULTILINE)
        text = re.sub(r'^Chapter\s+\d+.*?$', '', text, flags=re.MULTILINE)
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def prepare_data(self,
                combined_df=None,  # ← CHANGED: Take the combined DataFrame directly
                max_source_len=512,
                max_target_len=256,  # ← INCREASED: MCQs need more tokens
                batch_size=4,
                val_split=0.1):
        """
        FIXED: Prepare data loaders for training from combined dataset
        """
        if combined_df is None:
            raise ValueError("combined_df is required")

        # Split by task type
        summary_df = combined_df[combined_df['task'] == 'summarization'].reset_index(drop=True)
        mcq_df = combined_df[combined_df['task'] == 'mcq_generation'].reset_index(drop=True)

        source_texts = []
        summaries = []
        contexts = []
        questions = []
        answers = []
        subjects = []

        # Add summarization data
        if not summary_df.empty:
            source_texts = summary_df['text'].tolist()
            summaries = summary_df['ctext'].tolist()

        # Add MCQ data - FIXED COLUMN MAPPING
        if not mcq_df.empty:
            contexts = mcq_df['text'].tolist()      # ← FIXED: 'text' not 'context'
            questions = [None] * len(contexts)      # ← No pre-made questions
            answers = mcq_df['ctext'].tolist()      # ← FIXED: 'ctext' contains target MCQ format
            subjects = mcq_df['subject'].tolist()

        # Create dataset with all examples
        full_dataset = MultiTaskEducationalDataset(
            source_texts=source_texts,
            summaries=summaries,
            contexts=contexts,
            questions=questions,
            answers=answers,
            subjects=subjects,
            tokenizer=self.tokenizer,
            max_source_len=max_source_len,
            max_target_len=max_target_len  # ← INCREASED
        )

        # Split into train/val
        train_size = int((1 - val_split) * len(full_dataset))
        val_size = len(full_dataset) - train_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )

        # Create data loaders
        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer, model=self.model, label_pad_token_id=-100
        )

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=data_collator
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=data_collator
        )

        print(f"Train examples: {len(train_dataset)}, Validation examples: {len(val_dataset)}")
        return train_loader, val_loader

    def state_dict(self):
        # return exactly what torch.save needs
        return self.model.state_dict()

    def load_state_dict(self, state_dict):
        # delegate loading
        return self.model.load_state_dict(state_dict)

    def identify_subject_area(self, text: str) -> str:
        """Enhanced subject detection for all educational fields"""
        subject_patterns = {
            "mathematics": r'\b(equation|theorem|algebra|calculus|geometry|math|formula|polynomial|function|integral|derivative|proof|lemma|axiom|set|subset|relation|quantifier|logic|∀|∃|∈|⊆|∪|∩|⇒|⇔|≤|≥|∞|matrix|vector|graph|domain|range)\b',
            "biology": r'\b(cell|organism|DNA|RNA|protein|gene|chromosome|nucleus|membrane|cytoplasm|enzyme|bacteria|virus|evolution|species|tissue|organ|metabolism|photosynthesis|mitosis|meiosis|plasma|molecular|ribosome|mitochondria|chloroplast)\b',
            "chemistry": r'\b(molecule|atom|element|compound|reaction|chemical|bond|ion|electron|proton|neutron|periodic|solution|acid|base|catalyst|organic|inorganic|stoichiometry|thermodynamics|valence|oxidation|reduction|equilibrium|pH|molarity)\b',
            "physics": r'\b(force|energy|motion|gravity|velocity|acceleration|momentum|wave|frequency|amplitude|electric|magnetic|field|quantum|relativity|mass|charge|voltage|current|resistance|thermodynamics|mechanics|optics|nuclear)\b',
            "computer_science": r'\b(algorithm|programming|software|hardware|code|database|computation|binary|variable|class|method|interface|function|object|annotation|java|python|javascript|array|list|framework|API)\b',
            "history": r'\b(century|historical|ancient|medieval|civilization|revolution|dynasty|empire|king|queen|war|battle|treaty|colonial|independence|democracy|republic|monarch|feudal|renaissance|enlightenment|industrial|political|social|economic|cultural|period|era|chronological)\b',
            "literature": r'\b(novel|poem|author|literary|character|fiction|metaphor|narrator|plot|theme|symbolism|prose|verse|rhetoric|genre|style|narrative|protagonist|antagonist|poetry|drama|comedy|tragedy|allegory|irony|satire|romanticism|realism)\b',
            "economics": r'\b(economy|economic|market|price|demand|supply|inflation|GDP|recession|investment|capital|labor|productivity|competition|monopoly|trade|export|import|fiscal|monetary|budget|tax|revenue|profit|cost|efficiency|consumer|producer|industry|business|corporation|financial|banking|economics|macroeconomic|microeconomic|globalization)\b',
            "science": r'\b(biology|chemistry|physics|scientific|experiment|hypothesis|molecule|atom|cell|theory|research|study|analysis|observation|data|method|conclusion|evidence|variable|control)\b'
        }

        text_lower = text.lower()
        scores = {}

        for subject, pattern in subject_patterns.items():
            matches = re.findall(pattern, text_lower)
            scores[subject] = len(matches)

        detected = max(scores, key=scores.get)
        return detected if scores[detected] > 0 else "general"

"""Training the Model"""

def train(
        self,
        train_loader,
        val_loader=None,
        start_epoch: int = 0,
        epochs: int = 5,
        learning_rate: float = 5e-5,
        warmup_ratio: float = 0.1,
        gradient_accumulation_steps: int = 4,
        max_grad_norm: float = 1.0,
        checkpoint_dir: str = "./checkpoints",
        log_steps: int = 100
    ):
        os.makedirs(checkpoint_dir, exist_ok=True)
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        total_steps = (len(train_loader) * epochs) // gradient_accumulation_steps
        warmup_steps = int(warmup_ratio * total_steps)
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

        training_stats = {'train_losses': [], 'val_losses': [], 'learning_rates': []}

        for epoch in range(start_epoch, epochs):
            self.model.train()
            running_loss = 0.0
            steps = 0
            optimizer.zero_grad()

            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                loss = outputs.loss / gradient_accumulation_steps
                loss.backward()
                running_loss += loss.item() * gradient_accumulation_steps

                if (batch_idx + 1) % gradient_accumulation_steps == 0 or batch_idx == len(train_loader)-1:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    steps += 1

                    if steps % log_steps == 0:
                        avg_loss = running_loss / steps
                        lr = scheduler.get_last_lr()[0]
                        print(f"Epoch: {epoch+1}, Step: {steps}, Loss: {avg_loss:.6f}, LR: {lr:.8f}")
                        # checkpoint per log step if desired

            avg_train_loss = running_loss / steps
            training_stats['train_losses'].append(avg_train_loss)
            training_stats['learning_rates'].append(scheduler.get_last_lr()[0])
            print(f"Epoch {epoch+1} completed. Training Loss: {avg_train_loss:.6f}")

            if val_loader:
                val_loss = self.evaluate(val_loader)
                training_stats['val_losses'].append(val_loss)
                print(f"Validation Loss: {val_loss:.6f}")

            checkpoints = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict()
            }
            torch.save(
                checkpoints,
                os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pt')
            )
            print(f"Saved checkpoint_epoch_{epoch+1}.pt to {checkpoint_dir}")

        # Only final model saved once in main, so no saving here
        return training_stats

MultiTaskEducationalModel.train = train

"""Evaluating the Model"""

def evaluate(self, val_loader):
        self.model.eval()
        total_loss, valid_batches = 0.0, 0
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation"):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                if not torch.isnan(outputs.loss):
                    total_loss += outputs.loss.item()
                    valid_batches += 1
        return total_loss / valid_batches if valid_batches else float('inf')

MultiTaskEducationalModel.evaluate = evaluate

"""Generating a Summary"""

def extract_key_points(self, segment, subject=None):
    """
    SIMPLIFIED: Original logic with direct fixes for broken sentences only
    """
    if subject is None:
        subject = self.identify_subject_area(segment)

    # SMART: Clean segment with subject awareness
    segment = self._enhanced_pdf_text_cleaning(segment, subject)

    sentences = sent_tokenize(segment)
    key_points = []

    for sentence in sentences:
        # Smart cleaning for each sentence
        clean_sentence = self._enhanced_pdf_text_cleaning(sentence, subject)

        # SENTENCE FORMATTING FIXES - Make sentences well-formed and readable

        # Fix 1: Fix missing spaces between words
        clean_sentence = re.sub(r'([a-z])([A-Z])', r'\1 \2', clean_sentence)  # "wordWord" -> "word Word"
        clean_sentence = re.sub(r'([.!?])([A-Za-z])', r'\1 \2', clean_sentence)  # "word.Word" -> "word. Word"
        clean_sentence = re.sub(r'([a-z])([a-z])([A-Z])', r'\1\2 \3', clean_sentence)  # Fix compound words

        # Fix 2: Remove mathematical gibberish for non-math subjects
        if subject != 'mathematics':
            # Remove broken math expressions like "f(− ) = f( ) = , f(− ) = f( ) = "
            clean_sentence = re.sub(r'[f|g|h]\([^)]*\)\s*=\s*[f|g|h]\([^)]*\)\s*=\s*[,\s]*', '', clean_sentence)
            clean_sentence = re.sub(r'[∈∂∀∃∑∏∫∪∩⊆⊇⊂⊃≤≥≠≈∞±×÷√∇∆◦•]+', ' ', clean_sentence)
            # Remove broken variable expressions
            clean_sentence = re.sub(r'\b[A-Z]\s*=\s*[A-Z]\s*[+\-*/=]\s*[A-Z]\b', '', clean_sentence)

        # Fix 3: Add proper context to sentences starting with pronouns
        if clean_sentence.startswith('Each is ') or clean_sentence.startswith('Each are '):
            context_words = {
                'biology': 'Each cell',
                'mathematics': 'Each element',
                'computer_science': 'Each component',
                'history': 'Each period',
                'economics': 'Each factor'
            }
            replacement = context_words.get(subject, 'Each element')
            clean_sentence = re.sub(r'^Each (is|are) ', replacement + r' \1 ', clean_sentence)

        if clean_sentence.startswith('This is ') and not any(word in clean_sentence.lower() for word in ['defined', 'called', 'known']):
            context_words = {
                'biology': 'This biological process is',
                'mathematics': 'This mathematical concept is',
                'computer_science': 'This programming concept is',
                'history': 'This historical development is',
                'economics': 'This economic principle is'
            }
            replacement = context_words.get(subject, 'This concept is')
            clean_sentence = clean_sentence.replace('This is ', replacement + ' ', 1)

        # Fix 4: Clean up broken formatting and excessive punctuation
        clean_sentence = re.sub(r'[,]{2,}', ',', clean_sentence)  # Multiple commas -> single comma
        clean_sentence = re.sub(r'[.]{2,}', '.', clean_sentence)  # Multiple periods -> single period
        clean_sentence = re.sub(r'[;]{2,}', ';', clean_sentence)  # Multiple semicolons
        clean_sentence = re.sub(r'\s+', ' ', clean_sentence)      # Multiple spaces -> single space

        # Fix 5: Fix broken word combinations from PDF extraction
        # Common biological terms
        clean_sentence = re.sub(r'\bD\s+N\s+A\b', 'DNA', clean_sentence)
        clean_sentence = re.sub(r'\bR\s+N\s+A\b', 'RNA', clean_sentence)
        clean_sentence = re.sub(r'\bcyto\s+plasm\b', 'cytoplasm', clean_sentence, flags=re.IGNORECASE)
        clean_sentence = re.sub(r'\bmito\s+chondria\b', 'mitochondria', clean_sentence, flags=re.IGNORECASE)

        # Fix missing spaces after punctuation
        clean_sentence = re.sub(r'([,:;])([A-Za-z])', r'\1 \2', clean_sentence)

        # Fix 6: Ensure proper sentence structure
        # Remove incomplete endings
        if clean_sentence.endswith((' and', ' or', ' but', ' which', ' that', ' because', ' since')):
            clean_sentence = clean_sentence.rsplit(' ', 1)[0]

        # Fix broken sentence starts
        if clean_sentence.startswith(('And ', 'Or ', 'But ', 'Because ', 'Since ', 'If ', 'When ', 'Where ')):
            # Remove these problematic starters for cleaner sentences
            clean_sentence = re.sub(r'^(And|Or|But|Because|Since|If|When|Where)\s+', '', clean_sentence)

        # Fix 7: Improve readability by fixing common PDF extraction errors
        # Fix "word word" duplicates at the beginning
        words = clean_sentence.split()
        if len(words) >= 2 and words[0].lower() == words[1].lower():
            clean_sentence = ' '.join(words[1:])

        # Fix spacing around hyphens and special characters
        clean_sentence = re.sub(r'\s*-\s*', ' - ', clean_sentence)
        clean_sentence = re.sub(r'\s*:\s*', ': ', clean_sentence)

        # Clean up any remaining formatting issues
        clean_sentence = re.sub(r'\s+', ' ', clean_sentence).strip()

        # ORIGINAL CLEANING LOGIC (keep this exactly the same)
        clean_sentence = re.sub(r'\[\/?[^\]]*\]', '', clean_sentence).strip()
        clean_sentence = re.sub(r'\s+', ' ', clean_sentence)
        clean_sentence = re.sub(r'^[*\-•]+\s*', '', clean_sentence)
        clean_sentence = re.sub(r'^---+\s*', '', clean_sentence)

        # Remove author/name patterns and artifacts
        clean_sentence = re.sub(r'^[A-Z][a-z]+\s+[A-Z][a-z]+\s*\.?\d*\s*', '', clean_sentence)
        clean_sentence = re.sub(r'^\.?\d+\s*', '', clean_sentence)
        clean_sentence = re.sub(r'^\d+\.\d+\s*', '', clean_sentence)

        # Remove duplicate words at start
        words = clean_sentence.split()
        if len(words) >= 2 and words[0] == words[1] and words[0][0].isupper():
            clean_sentence = ' '.join(words[1:])

        clean_sentence = clean_sentence.strip()

        if len(clean_sentence) < 35 or len(clean_sentence.split()) < 8:
            continue

        # Skip questions
        if clean_sentence.strip().endswith('?'):
            continue

        # SMART: Only detect math garbage for non-math subjects
        if subject != 'mathematics':
            math_garbage_indicators = [
                r'[∈∂∀∃∑∏∫∪∩⊆⊇⊂⊃≤≥≠≈∞±]+',
                r'\b[A-Z]\s*=\s*[A-Z]',  # Variables like "A = B"
                r'\b\d+\s*[+\-*/=]\s*\d+',  # Math expressions
                r'\([0-9]+\)',  # Equation numbers like (9), (10)
                r'\b[cid:\d+]\b',  # CID references
                r'^[A-Z]\s+[A-Z]\s+[A-Z]',  # Broken spacing like "D D is"
            ]

            if any(re.search(pattern, clean_sentence) for pattern in math_garbage_indicators):
                continue

        # Skip implementation details (keep original logic)
        specific_indicators = [
            'we then use', 'you can use', 'for example', 'such as',
            'alternatively', 'in this case', 'here is', 'as shown',
            'the following', 'above example', 'below', 'this shows',
            'we can see', 'as we can see', 'we observe', 'we find',
            'let us', 'let\'s', 'consider the', 'look at the'
        ]

        if any(indicator in clean_sentence.lower() for indicator in specific_indicators):
            continue

        # Skip problematic starters (keep original logic)
        problematic_starters = [
            'this is because', 'to avoid this', 'for this reason', 'therefore', 'thus', 'hence',
            'as a result', 'consequently', 'in addition', 'furthermore', 'moreover', 'however',
            'on the other hand', 'in contrast', 'similarly', 'likewise', 'for example', 'for instance',
            'in other words', 'that is', 'namely', 'specifically', 'in particular', 'especially',
            'this means', 'this indicates', 'this suggests', 'this implies', 'this shows',
            'it is', 'it can be', 'it should be', 'it may be', 'it will be', 'it has been',
            'they are', 'they can be', 'they should be', 'they may be', 'they will be',
            'these are', 'those are', 'such', 'the following', 'the above', 'as mentioned',
            'anyway', 'also', 'additionally', 'meanwhile', 'besides', 'alternatively',
            'otherwise', 'instead', 'still', 'yet', 'nevertheless', 'nonetheless',
            'we then', 'you can', 'we can', 'one can', 'it allows', 'this allows'
        ]

        sentence_lower = clean_sentence.lower()
        is_problematic = any(sentence_lower.startswith(starter) for starter in problematic_starters)

        if is_problematic:
            continue

        # UNIVERSAL conceptual indicators (keep original logic)
        conceptual_indicators = [
            'is defined as', 'refers to', 'means that', 'is a type of', 'is a form of',
            'is a method', 'is a process', 'is a technique', 'is a concept', 'is a principle',
            'is a theory', 'is a law', 'is a rule', 'is a system', 'is a structure',
            'allows to', 'enables to', 'provides the ability', 'makes it possible',
            'is used to', 'is designed to', 'is intended to', 'serves to', 'functions to',
            'represents', 'constitutes', 'forms the basis', 'establishes',
            'differs from', 'compared to', 'in contrast to', 'unlike', 'similar to',
            'advantage of', 'benefit of', 'drawback of', 'limitation of', 'importance of',
            'key feature', 'main characteristic', 'primary purpose', 'essential component',
            'fundamental', 'basic', 'central', 'critical', 'significant', 'important',
            # Universal patterns for ALL subjects
            'emerged', 'developed', 'occurred', 'happened', 'began', 'started',
            'led to', 'resulted in', 'caused', 'established', 'founded', 'created',
            'characterized by', 'distinguished by', 'known for', 'recognized as',
            'consists of', 'contains', 'includes', 'features', 'comprises'
        ]

        has_conceptual_content = any(indicator in sentence_lower for indicator in conceptual_indicators)

        # ENHANCED: Universal definitional patterns (keep original logic)
        if not has_conceptual_content:
            definitional_patterns = [
                r'\b[A-Z][a-zA-Z]+\b.*\b(is|are|was|were)\b.*\b(that|which|used|designed|intended|responsible)\b',
                r'\b[A-Z][a-zA-Z]+\b.*\b(provides?|enables?|allows?|supports?|facilitates?)\b.*\b(for|to|by)\b',
                r'\b[A-Z][a-zA-Z]+\b.*\b(consists of|comprises?|contains?|includes?|features?)\b',
                # Historical patterns
                r'\b[A-Z][a-zA-Z]+\b.*\b(emerged|developed|occurred|happened|began)\b.*\b(in|during|throughout)\b',
                r'\b[A-Z][a-zA-Z]+\b.*\b(led to|resulted in|caused|established)\b',
                # Economic patterns
                r'\b[A-Z][a-zA-Z]+\b.*\b(affects|influences|determines|controls)\b.*\b(the|market|economy|price)\b',
                # Universal patterns
                r'\b[A-Z][a-zA-Z]+\b.*\b(can|may|might|will|must)\b.*\b(be|provide|create|cause|result)\b',
            ]

            is_definitional = any(re.search(pattern, clean_sentence, re.IGNORECASE) for pattern in definitional_patterns)

            if not is_definitional:
                continue

        # ENHANCED: Comprehensive subject terms for ALL fields (keep original logic)
        subject_terms = {
            'mathematics': [
                'theorem', 'proof', 'equation', 'formula', 'function', 'set', 'relation', 'property',
                'definition', 'lemma', 'axiom', 'corollary', 'variable', 'constant', 'expression',
                'integration', 'derivative', 'limit', 'matrix', 'vector', 'graph', 'domain', 'range',
                'algebra', 'calculus', 'geometry', 'polynomial', 'trigonometry', 'statistics', 'number'
            ],
            'biology': [
                'cell', 'organism', 'DNA', 'RNA', 'protein', 'gene', 'chromosome', 'nucleus',
                'membrane', 'tissue', 'organ', 'system', 'enzyme', 'molecule', 'structure',
                'function', 'process', 'metabolism', 'photosynthesis', 'respiration', 'evolution',
                'species', 'bacteria', 'virus', 'mitochondria', 'ecosystem', 'biodiversity',
                'biological', 'cellular', 'genetic', 'molecular', 'living', 'life', 'organelle'
            ],
            'chemistry': [
                'molecule', 'atom', 'element', 'compound', 'reaction', 'bond', 'electron',
                'proton', 'neutron', 'ion', 'solution', 'acid', 'base', 'catalyst', 'organic',
                'inorganic', 'periodic', 'valence', 'oxidation', 'reduction', 'equilibrium',
                'chemical', 'formula', 'equation', 'molar', 'concentration', 'pH', 'synthesis'
            ],
            'physics': [
                'force', 'energy', 'motion', 'velocity', 'acceleration', 'momentum', 'wave',
                'frequency', 'amplitude', 'field', 'particle', 'mass', 'charge', 'voltage',
                'current', 'resistance', 'quantum', 'relativity', 'gravity', 'thermodynamics',
                'mechanics', 'optics', 'nuclear', 'electromagnetic', 'radiation', 'wavelength'
            ],
            'computer_science': [
                'algorithm', 'data structure', 'object', 'class', 'interface', 'inheritance',
                'polymorphism', 'encapsulation', 'abstraction', 'framework', 'architecture',
                'design pattern', 'methodology', 'paradigm', 'principle', 'concept',
                'synchronization', 'concurrency', 'thread', 'process', 'database',
                'collection', 'exception', 'automation', 'optimization', 'performance',
                'application', 'system', 'software', 'programming', 'development'
            ],
            'history': [
                'century', 'historical', 'ancient', 'medieval', 'civilization', 'revolution', 'dynasty',
                'empire', 'king', 'queen', 'war', 'battle', 'treaty', 'colonial', 'independence',
                'democracy', 'republic', 'monarch', 'feudal', 'renaissance', 'enlightenment',
                'industrial', 'political', 'social', 'economic', 'cultural', 'period', 'era',
                'society', 'government', 'nation', 'state', 'ideology', 'movement', 'church',
                'religious', 'trade', 'commerce', 'people', 'population', 'world', 'europe',
                'asia', 'africa', 'america', 'power', 'control', 'influence', 'change'
            ],
            'economics': [
                'market', 'supply', 'demand', 'price', 'cost', 'profit', 'revenue', 'inflation',
                'GDP', 'interest', 'investment', 'consumption', 'production', 'trade', 'economy',
                'monetary', 'fiscal', 'policy', 'unemployment', 'growth', 'development', 'income',
                'wealth', 'capital', 'labor', 'resource', 'scarcity', 'utility', 'competition',
                'economic', 'financial', 'commercial', 'business', 'industry', 'finance'
            ],
            'science': [
                'cell', 'DNA', 'protein', 'organism', 'atom', 'molecule', 'reaction', 'energy',
                'system', 'process', 'structure', 'function', 'experiment', 'hypothesis',
                'theory', 'research', 'study', 'analysis', 'observation', 'principle'
            ]
        }

        relevant_terms = subject_terms.get(subject, subject_terms.get('science', []))
        has_domain_relevance = any(term in clean_sentence.lower() for term in relevant_terms)

        if not has_domain_relevance:
            continue

        # UNIVERSAL meaningful statement patterns (keep original logic)
        is_meaningful_statement = (
            # Definitional patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(is|are|was|were)\b.*\b(a|an|the)\b.*\b(that|which|used|designed|intended|responsible|known|type|form|method|process|technique|concept|principle|theory|law|system|structure)\b', clean_sentence, re.IGNORECASE) or
            # Functional patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(provides?|offers?|enables?|allows?|supports?|facilitates?|creates?|produces?|generates?|establishes?|forms?|constitutes?|represents?|demonstrates?|shows?|reveals?)\b', clean_sentence, re.IGNORECASE) or
            # Comparative patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(differs from|compared to|similar to|unlike|advantage of|benefit of|characteristic of|feature of|component of|part of|aspect of|element of)\b', clean_sentence, re.IGNORECASE) or
            # Capability patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(can|cannot|will|will not|must|must not|should|should not|may|might)\b.*\b(be|provide|handle|process|execute|manage|enable|allow|create|produce|demonstrate|show|result in|lead to)\b', clean_sentence, re.IGNORECASE) or
            # Importance patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(important|significant|essential|fundamental|critical|key|main|primary|central|basic|crucial)\b.*\b(for|to|in|because|since|as|due to)\b', clean_sentence, re.IGNORECASE) or
            # Historical/temporal patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(emerged|developed|occurred|happened|began|started|ended|led to|resulted in|caused|established|founded)\b', clean_sentence, re.IGNORECASE) or
            # Economic patterns
            re.search(r'\b[A-Z][a-zA-Z]+\b.*\b(affects|influences|determines|controls|impacts)\b.*\b(the|market|economy|price|demand|supply)\b', clean_sentence, re.IGNORECASE)
        )

        if not is_meaningful_statement:
            continue

        # PROGRAMMING STANDARD: Grammar check (keep original logic)
        doc = self.nlp(clean_sentence)
        has_subject = any(token.dep_ in ['nsubj', 'nsubjpass', 'csubj'] for token in doc)
        has_predicate = any(token.pos_ in ['VERB', 'AUX'] for token in doc)

        if not (has_subject and has_predicate):
            continue

        # PROGRAMMING STANDARD: Final formatting (keep original logic)
        if clean_sentence[0].islower():
            clean_sentence = clean_sentence[0].upper() + clean_sentence[1:]
        if not clean_sentence.endswith(('.', '!', '?')):
            clean_sentence += '.'

        clean_sentence = re.sub(r'\s+', ' ', clean_sentence).strip()

        # Ensure it's not a duplicate
        if clean_sentence not in key_points:
            key_points.append(clean_sentence)

    return key_points

# Replace the method in your class
MultiTaskEducationalModel.extract_key_points = extract_key_points

def generate_summary(self, text_segments: List[str], max_length: int = 300, min_length: int = 150) -> str:
    """
    Generate a coherent summary with minimal logging
    """
    self.model.eval()

    # Pre-process segments to remove noise
    cleaned_segments = []
    for segment in text_segments:
        cleaned = re.sub(r'[|]{2,}', '', segment)
        cleaned = re.sub(r'https?://\S+', '', cleaned)
        cleaned = re.sub(r'[\[\]{}]', '', cleaned)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        if re.match(r'^(\d+\.|\[\d+\]|\(?\d{4}\)?|References|Bibliography)', cleaned.strip()):
            continue
        if len(cleaned.split()) > 10:
            cleaned_segments.append(cleaned)

    # Detect document subject area
    full_text = " ".join(cleaned_segments[:30])
    subject_area = self.identify_subject_area(full_text)

    # For very long content, extract key segments
    if len(cleaned_segments) > 10:
        selected_segments = []
        selected_segments.extend(cleaned_segments[:min(3, len(cleaned_segments))])

        for i, segment in enumerate(cleaned_segments[3:], 3):
            if (any(s.isupper() for s in segment.split()[:3]) or
                any(segment.startswith(word) for word in ['The', 'This', 'A ', 'In ', 'On ', 'As ']) or
                re.match(r'^\d+[\.\)]\s+\w+', segment) or
                any(phrase in segment.lower() for phrase in [
                    'importantly', 'significant', 'key', 'crucial', 'essential',
                    'primary', 'major', 'fundamental', 'critical', 'central',
                    'conclude', 'summary', 'in conclusion', 'therefore', 'thus',
                    'demonstrate', 'show', 'reveal', 'find', 'discover'
                ])):
                selected_segments.append(segment)

        selected_segments.extend(cleaned_segments[-3:])

        if len(selected_segments) > 15:
            densities = []
            for seg in selected_segments:
                doc = self.nlp(seg)
                content_words = sum(1 for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'PROPN', 'NUM'])
                total_words = len([token for token in doc if not token.is_punct])
                density = content_words / max(1, total_words)
                densities.append(density)

            paired = [(i, d) for i, d in enumerate(densities)]
            sorted_pairs = sorted(paired, key=lambda x: x[1], reverse=True)
            top_indices = [i for i, _ in sorted_pairs[:10]]

            must_include = list(range(min(3, len(selected_segments))))
            must_include.extend(range(max(0, len(selected_segments)-3), len(selected_segments)))

            final_indices = sorted(set(top_indices + must_include))
            selected_segments = [selected_segments[i] for i in final_indices]
    else:
        selected_segments = cleaned_segments

    # Extract key points from selected segments
    key_points = []
    for segment in selected_segments:
        segment_key_points = self.extract_key_points(segment)
        for point in segment_key_points:
            if point not in key_points:  # Avoid duplicates
                key_points.append(point)

    # Deduplicate and limit key points
    unique_key_points = []

    if key_points:
        point_embeddings = self.embedder.encode(key_points, convert_to_numpy=True, normalize_embeddings=True)
        selected_indices = [0]
        similarity_matrix = np.dot(point_embeddings, point_embeddings.T)

        for _ in range(min(20, len(key_points)-1)):  # Reduced from 25 to 20
            if not selected_indices:
                break
            avg_sim = np.mean([similarity_matrix[i, :] for i in selected_indices], axis=0)
            candidates = [i for i in range(len(key_points)) if i not in selected_indices]
            if not candidates:
                break
            next_idx = min(candidates, key=lambda i: avg_sim[i])
            selected_indices.append(next_idx)

        selected_indices.sort()
        unique_key_points = [key_points[i] for i in selected_indices]

    # Format as bullet points with improved structure
    if unique_key_points:
        summary = "SUMMARY:\n"
        for point in unique_key_points:
            clean_point = point.strip()
            if clean_point.startswith('- '):
                clean_point = clean_point[2:]
            if not clean_point.endswith(('.', '!', '?')):
                clean_point += '.'
            summary += f"- {clean_point}\n"
        return summary
    else:
        # Fallback to original method with reduced parameters
        prompt = (
            f"[SUMMARIZE] [SUBJECT:{subject_area}] Create a clear, coherent summary of the following "
            f"educational content. Focus on main concepts and key information. "
            f"Content: " + " ".join(selected_segments)
        )

        inputs = self.tokenizer(
            prompt,
            max_length=1024,
            truncation=True,
            return_tensors='pt'
        ).to(self.device)

        result = self.model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=max_length,
            min_length=min_length,
            num_beams=4,
            length_penalty=1.0,
            no_repeat_ngram_size=3,
            early_stopping=True,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

        summary = self.tokenizer.decode(result[0], skip_special_tokens=True)

        # Convert to bullet points
        sentences = sent_tokenize(summary)
        if sentences:
            bullet_summary = "SUMMARY:\n"
            for sent in sentences:
                if len(sent) > 20:
                    bullet_summary += f"- {sent}\n"
            return bullet_summary

        return summary

def _select_best_summary(self, candidates, subject_area):
    """
    Select the best summary from multiple candidates
    """
    if not candidates:
        return ""

    if len(candidates) == 1:
        return candidates[0]

    # Score each candidate on several dimensions
    scores = []

    for summary in candidates:
        score = 0
        sentences = sent_tokenize(summary)

        # 1. Appropriate length (number of sentences)
        if 4 <= len(sentences) <= 10:
            score += 2
        elif 3 <= len(sentences) <= 15:
            score += 1

        # 2. Sentence length variety (more natural)
        sent_lengths = [len(s) for s in sentences]
        length_variance = np.var(sent_lengths) if len(sent_lengths) > 1 else 0
        if length_variance > 200:  # Some variety but not extreme
            score += 1

        # 3. Contains subject-specific terminology
        if subject_area in ["mathematics", "physics", "computer_science"]:
            tech_terms = ["function", "algorithm", "equation", "value", "variable", "system", "method"]
            if any(term in summary.lower() for term in tech_terms):
                score += 1
        elif subject_area in ["literature", "history"]:
            human_terms = ["author", "work", "character", "period", "era", "narrative", "historical"]
            if any(term in summary.lower() for term in human_terms):
                score += 1

        # 4. Has good structure indicators
        structure_terms = ["first", "second", "then", "next", "finally", "in conclusion", "therefore"]
        if any(term in summary.lower() for term in structure_terms):
            score += 1

        # 5. Penalize artifacts and formatting issues
        if re.search(r'\[|\]|\*|"|\\|/|@|#|%|\|', summary):
            score -= 2

        scores.append(score)

    # Return the highest scoring candidate
    best_idx = scores.index(max(scores))
    return candidates[best_idx]

def _enhance_summary_structure(self, summary: str, subject: str) -> str:
    """
    Enhanced post-processing to improve summary quality with longer, more structured sentences
    """
    # Check if summary is in bullet point format
    if "SUMMARY:" in summary and "- " in summary:
        lines = summary.split("\n")
        bullet_points = []
        header = None

        # Extract header and bullet points
        for line in lines:
            line = line.strip()
            if line == "SUMMARY:":
                header = line
            elif line.startswith("- "):
                bullet_points.append(line[2:].strip())

        # Group related bullet points where possible for longer sentences
        points = []
        i = 0
        while i < len(bullet_points):
            current = bullet_points[i]
            if i+1 < len(bullet_points):
                next_point = bullet_points[i+1]
                # Check if points might be related
                if (len(current.split()) + len(next_point.split())) < 30:
                    # Try to combine related points with connector phrases
                    if any(w in next_point.lower() for w in ["it", "this", "these", "they", "such"]):
                        combined = f"{current} {next_point}"
                        points.append(combined)
                        i += 2
                        continue
                    # Try to combine with appropriate conjunctions
                    if not current.endswith(('.', '!', '?')):
                        current = current + '.'
                    if any(w in current.lower() for w in ["use", "enable", "allow", "provide"]) and \
                       any(w in next_point.lower() for w in ["use", "enable", "allow", "provide"]):
                        combined = f"{current} Similarly, {next_point}"
                        points.append(combined)
                        i += 2
                        continue

            # If no combinations made, add the single point
            points.append(current)
            i += 1

        # Rebuild the summary with enhanced structure
        structured_summary = header + "\n" if header else "SUMMARY:\n"

        # Group points into categories for subject-specific structuring
        if subject == "computer_science" and len(points) > 5:
            # Try to categorize points
            classes_methods = []
            data_structures = []
            general_concepts = []

            for point in points:
                lower_point = point.lower()
                if any(term in lower_point for term in ["class", "method", "interface", "annotation"]):
                    classes_methods.append(point)
                elif any(term in lower_point for term in ["map", "list", "set", "collection", "array"]):
                    data_structures.append(point)
                else:
                    general_concepts.append(point)

            # Add headers for each category with points
            if classes_methods:
                structured_summary += "- Classes and Methods:\n"
                for point in classes_methods:
                    structured_summary += f"  - {point}\n"

            if data_structures:
                structured_summary += "- Data Structures:\n"
                for point in data_structures:
                    structured_summary += f"  - {point}\n"

            if general_concepts:
                structured_summary += "- General Concepts:\n"
                for point in general_concepts:
                    structured_summary += f"  - {point}\n"
        else:
            # Just use enhanced points with better formatting
            for point in points:
                # Ensure proper sentence structure
                if point and point[0].isalpha():
                    point = point[0].upper() + point[1:]
                if not point.endswith(('.', '!', '?')):
                    point += '.'
                structured_summary += f"- {point}\n"

        return structured_summary

    # For non-bullet point summaries
    return summary

MultiTaskEducationalModel.generate_summary = generate_summary
MultiTaskEducationalModel._select_best_summary = _select_best_summary
MultiTaskEducationalModel._enhance_summary_structure = _enhance_summary_structure

def _validate_summary(self, summary: str) -> str:
    """
    ENHANCED validation with better sentence formation and readability
    """
    # Check if summary is in bullet point format
    is_bullet_format = "SUMMARY:" in summary and "- " in summary

    if is_bullet_format:
        # Split into lines to process header and bullet points separately
        lines = summary.split("\n")
        header = ""
        bullet_points = []

        for line in lines:
            line = line.strip()
            if line == "SUMMARY:":
                header = line
            elif line.startswith("- "):
                # Clean the bullet point text
                point = line[2:].strip()  # Remove "- " prefix

                # Apply enhanced cleaning
                point = self._enhanced_universal_cleaning(point)

                # CRITICAL: Additional sentence-level fixes for better readability
                # Fix broken sentence beginnings
                if point.startswith('Ex post:'):
                    point = 'Ex post analysis shows that ' + point[8:].strip()
                elif point.startswith('One also'):
                    point = re.sub(r'^One also.*?Unlike\s*,?', 'The average propensity to consume', point)

                # Fix incomplete sentences
                if point.endswith('up .'):
                    point = point[:-4] + 'establishment.'
                elif point.endswith('up'):
                    point = point + ' to full development.'

                # Ensure proper sentence structure
                if len(point) > 15 and point.count(' ') >= 3:
                    # Fix capitalization
                    if point and point[0].isalpha():
                        point = point[0].upper() + point[1:]

                    # Fix ending punctuation
                    if not point.endswith(('.', '!', '?')):
                        point += '.'

                    # Additional readability fixes
                    point = re.sub(r'\s+', ' ', point)  # Normalize spaces
                    point = re.sub(r'([.!?])\s*([a-z])', r'\1 \2', point)  # Fix spacing after punctuation

                    # Only add if it's a meaningful, complete sentence
                    if (not re.search(r'[\\/%{}#@]', point) and  # No special chars
                        not re.match(r'^(References|Bibliography|Footnotes|Citations)', point) and  # Not references
                        len(point.split()) >= 5):  # At least 5 words
                        bullet_points.append(point)

        # If we found valid bullet points, rebuild the summary
        if bullet_points:
            clean_summary = header + "\n"
            for point in bullet_points:
                clean_summary += f"- {point}\n"
            return clean_summary

    # Standard validation for non-bullet format with enhanced cleaning
    summary = self._enhanced_universal_cleaning(summary)

    # Only include complete sentences
    sentences = sent_tokenize(summary)
    valid_sentences = []
    for sent in sentences:
        # Apply same cleaning to individual sentences
        sent = self._enhanced_universal_cleaning(sent)

        # Basic validation criteria
        if (len(sent) > 15 and  # Not too short
            sent.count(' ') >= 4 and  # Has at least several words
            not re.search(r'[\\/%{}#@]', sent) and  # No special chars
            not re.match(r'^(References|Bibliography|Footnotes|Citations)', sent) and  # Not references
            re.search(r'^[A-Z]', sent)):  # Starts with capital

            # Ensure sentence ends with proper punctuation
            if not sent.endswith(('.', '!', '?')):
                sent += '.'

            valid_sentences.append(sent)

    # Convert to bullet point format for consistency
    if valid_sentences:
        bullet_summary = "SUMMARY:\n"
        for sent in valid_sentences:
            bullet_summary += f"- {sent}\n"
        return bullet_summary

    # If nothing valid found, return a minimal valid summary
    if summary and len(summary) > 50:
        # Try to extract at least one reasonable sentence
        clean_summary = re.sub(r'\s+', ' ', summary).strip()
        if len(clean_summary) > 0:
            return f"SUMMARY:\n- {clean_summary}.\n"

    # Completely invalid summary, return default
    return "SUMMARY:\n- No valid summary could be generated for this content.\n"

MultiTaskEducationalModel._validate_summary = _validate_summary

def _enhanced_universal_cleaning(self, text):
    """
    UNIVERSAL: Sentence-level cleaning that prevents formatting issues
    """
    # Apply the same universal word separation logic
    text = re.sub(r'([a-z])([A-Z])', r'\1 \2', text)
    text = re.sub(r'([.!?,:;])([A-Za-z])', r'\1 \2', text)

    # Universal long word handling (same logic as above)
    def smart_word_split(match):
        word = match.group(0)
        if len(word) > 20 and word.islower():
            result = word
            for pattern in ['therefore', 'validity', 'assessment', 'limited', 'expected', 'level', 'market', 'cannot']:
                if pattern in result and len(result) > len(pattern) + 3:
                    pos = result.find(pattern)
                    if pos >= 0 and pos + len(pattern) < len(result):
                        before = result[:pos + len(pattern)]
                        after = result[pos + len(pattern):]
                        if len(after) > 2:
                            result = before + ' ' + after
                            break
            return result
        return word

    text = re.sub(r'\b[a-z]{15,}\b', smart_word_split, text)

    # Universal sentence starter fixes
    text = re.sub(r'^Is\s+(empty|not\s+empty),', r'If it is \1,', text)
    text = re.sub(r'^([A-Z][a-z]+):\s*([A-Z])', r'\1 is \2', text)
    text = re.sub(r'^([A-Z][a-z]+):\s*([a-z])', r'\1 is \2', text)

    # Universal technical term fixes
    text = re.sub(r'\bis\s+Present\(\)', 'isPresent()', text)
    text = re.sub(r'\b([A-Z][a-z]+)\s+([A-Z][a-z]+)\s+Exception\b', r'\1\2Exception', text)

    # Standard cleanup (keep all existing cleanup logic)
    text = re.sub(r"'source':\s*'[^']*'", '', text)
    text = re.sub(r'\[\/?[^\]]*\]', '', text)
    text = re.sub(r'[\\/%{}#@]+', '', text)
    text = re.sub(r'"{2,}', '"', text)
    text = re.sub(r"'{2,}", "'", text)

    # Fix incomplete endings
    text = re.sub(r'\bup\s*\.$', 'establishment.', text)
    text = re.sub(r'\bto\s*\.$', 'completion.', text)
    text = re.sub(r'\band\s*\.$', '.', text)

    # Citations and references
    text = re.sub(r'\(\d{4}\)', '', text)
    text = re.sub(r'\[\d+\]', '', text)
    text = re.sub(r'\([^)]*et\s+al[^)]*\)', '', text, flags=re.IGNORECASE)

    # Page markers
    text = re.sub(r'^[A-Z][a-z]+\s+[A-Z][a-z]+\s*\.?\d*\s*', '', text)
    text = re.sub(r'^\d+\.\d+\s*', '', text)
    text = re.sub(r'^Chapter\s+\d+\s*', '', text, flags=re.IGNORECASE)

    # URLs
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'www\.\S+', '', text)

    # Final whitespace cleanup
    text = re.sub(r'\s+', ' ', text).strip()

    return text

MultiTaskEducationalModel._enhanced_universal_cleaning = _enhanced_universal_cleaning

"""Generate Questions and Distractors"""

def _shorten_sentence(self, text, max_words=15):
    """
    SAFE: Clean question endings without breaking anything
    """
    try:
        if not text:
            return text

        words = text.split()
        if len(words) <= max_words:
            return text.strip()

        # Only fix the "..." issue for questions
        if text.strip().endswith('?'):
            # Look for natural breaking points
            breaking_phrases = ["in the case of", " when ", " that ", " which ", " where "]
            for phrase in breaking_phrases:
                if phrase in text.lower():
                    break_idx = text.lower().find(phrase)
                    if break_idx > 0 and break_idx < len(text) * 0.7:  # Don't break too late
                        shortened = text[:break_idx].strip()
                        if len(shortened.split()) >= 4:
                            return shortened + "?"

        # Default: just remove "..." if present
        result = ' '.join(words[:max_words]).strip()
        if result.endswith('...'):
            result = result[:-3].strip()
        return result

    except Exception:
        return text  # Return original if anything fails

def generate_mcq_from_content(self, context, subject="general", key_entities=None, focus_concept=None):
    """
    Debug version with detailed logging - NLP ONLY
    """
    try:
        self.model.eval()
        clean_context = self._clean_context_for_generation(context)

        # Prevent duplicate questions from the same segment
        context_hash = hash(clean_context[:300])
        if not hasattr(self, '_used_contexts'):
            self._used_contexts = set()
        if context_hash in self._used_contexts:
            return None
        self._used_contexts.add(context_hash)

        prompt_templates = [
            f"Generate a concise technical multiple choice question about a key concept or feature from this content. Include 4 options: one correct and three plausible distractors, all related to the same academic domain.\n\nContent: {clean_context}\n\nQuestion:",
            f"Create a multiple choice question asking about the purpose, function, or use of a specific concept mentioned. Provide 4 options: only one correct, the rest are plausible but incorrect.\n\nContent: {clean_context}\n\nQuestion:",
        ]

        for i, prompt in enumerate(prompt_templates):
            inputs = self.tokenizer(
                prompt,
                max_length=450,
                truncation=True,
                return_tensors='pt'
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    max_length=180,
                    min_length=50,
                    num_beams=4,
                    length_penalty=1.1,
                    no_repeat_ngram_size=3,
                    early_stopping=True,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            mcq = self._parse_generated_mcq(generated_text, subject)
            if not mcq:
                continue

            if not self._validate_final_mcq(mcq):
                continue

            if not self._is_good_question_type(mcq['question']):
                continue

            try:
                correct_answer_text = mcq['options'][mcq['correct_answer']]
            except (KeyError, TypeError):
                continue

            # Generate contextual distractors using NLP ONLY
            distractors = self._generate_contextual_options(
                correct_answer_text, clean_context, subject
            )

            # STRICT: Must have at least 2 distractors from NLP, otherwise FAIL
            if len(distractors) < 2:
                continue

            # Take up to 3 distractors from NLP only
            distractors = distractors[:3]

            # Create final options with proper formatting
            options, correct_key = self._finalize_options(correct_answer_text, distractors)

            # Clean the question
            try:
                question = self._shorten_sentence(mcq['question'], 15)
                if not question.endswith('?'):
                    question += '?'
            except Exception:
                question = mcq['question']

            # Final validation
            final_mcq = {
                "question": question,
                "options": options,
                "correct_answer": correct_key
            }

            if self._validate_final_mcq(final_mcq):
                return final_mcq

        return None

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None

def _is_good_question_type(self, question):
    q = question.lower().strip()
    bad_patterns = [
        "what is the main idea", "main idea of the passage", "main topic", "best title", "according to the passage",
        "according to the text", "as stated in the passage", "in the passage", "in the text"
    ]
    # Accept only if not meta and not super long
    if any(bp in q for bp in bad_patterns):
        return False
    if len(q.split()) > 18 or not q.endswith("?"):
        return False
    # Should start with "what", "why", "which", etc.
    start_ok = any(q.startswith(x) for x in ["what", "why", "how", "which", "when", "where"])
    return start_ok

def _is_content_relevant(self, mcq, context, subject):
    """
    ENHANCED: Better content relevance checking for all subjects
    """
    question = mcq.get('question', '').lower()
    correct_answer = mcq['options'][mcq['correct_answer']].lower()
    context_lower = context.lower()

    # Extract key terms from context
    try:
        from nltk.tokenize import word_tokenize
        from nltk.corpus import stopwords

        context_words = word_tokenize(context_lower)
        stop_words_set = set(stopwords.words('english'))
        context_terms = {word for word in context_words if word.isalpha() and len(word) > 2 and word not in stop_words_set}
    except:
        # Fallback if NLTK not available
        import re
        context_words = re.findall(r'\b[a-zA-Z]{3,}\b', context_lower)
        common_words = {'the', 'and', 'are', 'for', 'this', 'that', 'with', 'have', 'will', 'can', 'all', 'but', 'not', 'you', 'they', 'was', 'his', 'her', 'she', 'him', 'our', 'one', 'two', 'may', 'use', 'get', 'new', 'now', 'old', 'see', 'way', 'who', 'its', 'did', 'yes', 'how', 'man', 'day', 'too', 'any', 'why', 'let', 'put', 'say', 'she', 'try', 'ask', 'run', 'own', 'end', 'why', 'let', 'old', 'try'}
        context_terms = {word for word in context_words if word not in common_words}

    # Check for overlap between question/answer and context
    import re
    question_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', question))
    answer_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', correct_answer))

    qa_words = question_words.union(answer_words)
    overlap = len(context_terms.intersection(qa_words))

    # Must have meaningful overlap with context
    if overlap < 1:  # More lenient than biology (was 2)
        return False

    # Must contain subject-relevant terminology
    if not self._contains_subject_terms(question + ' ' + correct_answer, subject):
        return False

    return True

def _clean_context_for_generation(self, context):
    """Clean context for MCQ generation"""
    # Remove artifacts and page markers
    context = re.sub(r'---\s*---.*?---\s*', '', context, flags=re.MULTILINE)
    context = re.sub(r'^\d+\s*/\s*\d+.*?$', '', context, flags=re.MULTILINE)
    context = re.sub(r'^Luca Lezzerini.*?$', '', context, flags=re.MULTILINE)
    context = re.sub(r'^\s*\.\d+\s*$', '', context, flags=re.MULTILINE)

    # Normalize whitespace
    context = re.sub(r'\s+', ' ', context).strip()

    # Limit context length for training compatibility
    words = context.split()
    if len(words) > 200:
        context = ' '.join(words[:200])

    return context

def _parse_generated_mcq(self, generated_text, subject):
    """
    FIXED: Use EXACT biology logic for ALL subjects
    """
    try:
        text = generated_text.strip()

        mcq_data = {}

        # STEP 1: Extract question (EXACT same as biology)
        question_patterns = [
            r'Question:\s*([^?]+\?)',
            r'(?:^|\n)((?:What|Which|How|Why|When|Where)[^?]+\?)',
            r'([A-Z][^?]*\?)',
        ]

        question_found = False
        for pattern in question_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                question = match.strip()
                question = re.sub(r'^(Question|Q):\s*', '', question, flags=re.IGNORECASE)

                if len(question.split()) >= 4 and question.endswith('?'):
                    mcq_data['question'] = question
                    question_found = True
                    break
            if question_found:
                break

        # STEP 2: Extract answer (EXACT same patterns as biology)
        correct_answer_text = None

        answer_patterns = [
            # EXACT biology pattern that works
            r'Correct Answer:\s*([^.\n]+?)(?:\s+Wrong|\s+WRONG|\s+Www|\s+Ww|\n|$)',

            # Additional robust patterns
            r'Correct Answer:\s*([^,\n]+?)(?:\s+Wrong|\n|$)',
            r'Answer:\s*([^.\n]+?)(?:\s+Wrong|\n|$)',
            r'Correct Answer:\s*([^,\n]+?),',
            r'Correct Answer:\s*([^.]+?)\.(?:\s+Wrong|\s+WRONG|\n)',
        ]

        for pattern in answer_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            if match:
                answer = match.group(1).strip()

                # Clean exactly like biology
                answer = re.sub(r'\s+', ' ', answer)
                answer = re.sub(r'[^\w\s\-\(\)\.\/\,\:\;]', '', answer).strip()
                answer = answer.replace(' .', '').replace(' ,', '').strip()

                # Remove artifacts
                answer = re.sub(r'(Wrong|WRONG|Www|Ww).*$', '', answer, flags=re.IGNORECASE).strip()
                answer = re.sub(r'Option\s+\d+.*$', '', answer, flags=re.IGNORECASE).strip()

                # Validate exactly like biology
                if (len(answer) > 1 and len(answer) < 200 and
                    not answer.lower().startswith(('wrong', 'option', 'choice', 'www', 'ww', 'correct')) and
                    not answer.lower().endswith(('wrong', 'option', 'choice'))):

                    correct_answer_text = answer
                    break

        # STEP 3: Build MCQ if both extracted
        if mcq_data.get('question') and correct_answer_text:

            mcq = {
                "question": mcq_data['question'],
                "options": {
                    "A": correct_answer_text,
                    "B": "temp_option_1",
                    "C": "temp_option_2",
                    "D": "temp_option_3"
                },
                "correct_answer": "A"
            }

            return mcq

        return None

    except Exception as e:
        return None

def _is_important_question(self, question, context, subject):
    """
    ENHANCED: Filter out generic questions and keep only conceptual ones
    """
    question_lower = question.lower()

    # ENHANCED: More comprehensive rejection of generic/meta questions
    generic_patterns = [
        'what is the best title', 'what is the main topic', 'what is the passage about',
        'what is the author', 'what is discussed', 'which statement best',
        'according to the passage', 'the passage states', 'mentioned in the text',
        'what does the passage', 'the text explains', 'as stated in',
        'best title for', 'main idea of', 'central theme of',
        'passage discusses', 'text is about', 'article focuses on',
        'reading is about', 'paragraph explains', 'section describes'
    ]

    if any(pattern in question_lower for pattern in generic_patterns):
        return False

    # REJECT: Overly specific implementation questions
    specific_patterns = [
        'what method', 'which method', 'what syntax', 'which syntax',
        'what code', 'which code', 'what line', 'which line',
        'in the example', 'in this case', 'for this specific'
    ]

    if any(pattern in question_lower for pattern in specific_patterns):
        return False

    # ACCEPT: High-value conceptual questions
    conceptual_patterns = [
        'what is', 'what are', 'what does', 'what do',
        'how does', 'how do', 'how can', 'how is',
        'why is', 'why are', 'why does', 'why do',
        'when is', 'when are', 'when does', 'when do',
        'where is', 'where are', 'where does', 'where do',
        'which of the following', 'what is the difference between',
        'what is the relationship between', 'what is the purpose of',
        'what is the advantage of', 'what is the disadvantage of',
        'what characterizes', 'what defines', 'what distinguishes'
    ]

    has_conceptual_start = any(question_lower.startswith(pattern) for pattern in conceptual_patterns)

    if not has_conceptual_start:
        return False

    # Must contain subject-relevant terminology
    subject_terms = {
        'computer_science': [
            'class', 'object', 'interface', 'inheritance', 'polymorphism', 'encapsulation',
            'algorithm', 'data structure', 'thread', 'process', 'exception', 'collection',
            'framework', 'design pattern', 'concurrency', 'synchronization', 'automation'
        ],
        'mathematics': [
            'function', 'equation', 'theorem', 'proof', 'set', 'relation', 'matrix',
            'vector', 'derivative', 'integral', 'limit', 'algebra', 'geometry', 'calculus'
        ],
        'biology': [
            'cell', 'organism', 'DNA', 'protein', 'gene', 'evolution', 'metabolism',
            'photosynthesis', 'ecosystem', 'species', 'tissue', 'organ', 'enzyme'
        ],
        'chemistry': [
            'molecule', 'atom', 'reaction', 'compound', 'element', 'bond', 'catalyst',
            'solution', 'acid', 'base', 'oxidation', 'reduction', 'equilibrium'
        ],
        'physics': [
            'force', 'energy', 'motion', 'wave', 'field', 'particle', 'momentum',
            'acceleration', 'velocity', 'gravity', 'quantum', 'relativity'
        ],
        'history': [
            'civilization', 'revolution', 'empire', 'democracy', 'feudalism',
            'renaissance', 'enlightenment', 'colonialism', 'nationalism'
        ],
        'literature': [
            'narrative', 'character', 'theme', 'symbolism', 'metaphor', 'genre',
            'protagonist', 'plot', 'rhetoric', 'irony', 'allegory'
        ]
    }

    relevant_terms = subject_terms.get(subject, subject_terms.get('science', []))
    has_subject_relevance = any(term in question_lower for term in relevant_terms)

    return has_subject_relevance


MultiTaskEducationalModel._shorten_sentence = _shorten_sentence
MultiTaskEducationalModel.generate_mcq_from_content = generate_mcq_from_content
MultiTaskEducationalModel._is_good_question_type = _is_good_question_type
MultiTaskEducationalModel._clean_context_for_generation = _clean_context_for_generation
MultiTaskEducationalModel._parse_generated_mcq = _parse_generated_mcq
MultiTaskEducationalModel._is_important_question = _is_important_question
MultiTaskEducationalModel._is_content_relevant = _is_content_relevant

def _generate_contextual_options(self, correct_answer, context, subject):
    """
    Generate options and apply sentence case formatting
    """
    try:

        doc_answer = self.nlp(correct_answer)
        doc_context = self.nlp(context)

        distractors = []

        # Strategy 1: Entity-based substitution
        answer_entities = self._extract_key_entities(doc_answer)
        context_entities = self._extract_key_entities(doc_context)

        entity_options = self._generate_entity_based_options(
            correct_answer, answer_entities, context_entities, subject
        )
        distractors.extend(entity_options)

        # Strategy 2: Semantic similarity
        if len(distractors) < 3:
            semantic_options = self._generate_semantic_similarity_options(
                correct_answer, context, doc_answer
            )
            distractors.extend(semantic_options)

        # Strategy 3: Context-based extraction
        if len(distractors) < 3:
            context_options = self._generate_context_based_options(
                correct_answer, context, doc_context
            )
            distractors.extend(context_options)

        # Clean and validate options
        clean_distractors = []
        for i, option in enumerate(distractors):
            cleaned = self._clean_option_text(option)

            is_valid = self._is_valid_option(cleaned, correct_answer)

            if is_valid:
                clean_distractors.append(cleaned)
                if len(clean_distractors) >= 3:
                    break

        # APPLY SENTENCE CASE FORMATTING
        formatted_distractors = self._apply_sentence_case_to_all_options(clean_distractors[:3])

        return formatted_distractors

    except Exception as e:
        import traceback
        traceback.print_exc()
        return []

def _apply_sentence_case(self, option):
    """
    Convert option to proper sentence case: first word capitalized, rest lowercase
    """
    if not option:
        return option

    # Split into words
    words = option.split()
    if not words:
        return option

    # Process each word
    formatted_words = []
    for i, word in enumerate(words):
        if i == 0:
            # First word: capitalize first letter, rest lowercase
            formatted_word = word[0].upper() + word[1:].lower() if len(word) > 1 else word.upper()
        else:
            # All other words: lowercase
            formatted_word = word.lower()

        formatted_words.append(formatted_word)

    result = ' '.join(formatted_words)

    # Preserve period if original had one
    if option.endswith('.') and not result.endswith('.'):
        result += '.'

    return result

def _apply_sentence_case_to_all_options(self, distractors):
    """
    Apply sentence case to all distractor options
    """
    formatted_distractors = []

    for distractor in distractors:
        formatted = self._apply_sentence_case(distractor)
        formatted_distractors.append(formatted)

    return formatted_distractors

def _extract_key_entities(self, doc):
    """
    Extract meaningful entities and concepts from text using spaCy
    """
    entities = {
        'persons': [],
        'organizations': [],
        'locations': [],
        'concepts': [],
        'numbers': [],
        'dates': [],
        'technical_terms': []
    }

    # Named entities
    for ent in doc.ents:
        if ent.label_ in ['PERSON']:
            entities['persons'].append(ent.text)
        elif ent.label_ in ['ORG']:
            entities['organizations'].append(ent.text)
        elif ent.label_ in ['LOC', 'GPE']:
            entities['locations'].append(ent.text)
        elif ent.label_ in ['DATE', 'TIME']:
            entities['dates'].append(ent.text)
        elif ent.label_ in ['CARDINAL', 'ORDINAL', 'QUANTITY']:
            entities['numbers'].append(ent.text)

    # Key nouns and noun phrases (concepts)
    for token in doc:
        if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop and len(token.text) > 2:
            entities['concepts'].append(token.text)
        # Technical terms (often capitalized or compound)
        if token.pos_ == 'NOUN' and (token.text[0].isupper() or '_' in token.text):
            entities['technical_terms'].append(token.text)

    # Noun chunks for compound concepts
    for chunk in doc.noun_chunks:
        if 2 <= len(chunk.text.split()) <= 4:  # Meaningful compound terms
            entities['concepts'].append(chunk.text)

    # Remove duplicates and clean
    for key in entities:
        entities[key] = list(set([item.strip() for item in entities[key] if len(item.strip()) > 1]))

    return entities

def _generate_entity_based_options(self, correct_answer, answer_entities, context_entities, subject):
    """
    ENHANCED: Better subject detection and diversity for biology
    """
    options = []

    # CRITICAL: Force economics detection if economic terms present
    if any(term in correct_answer.lower() for term in ['economic', 'cost', 'revenue', 'market', 'competition', 'deficit']):
        subject = 'economics'

    # For sentence answers starting with "To", create meaningful alternatives
    if correct_answer.lower().startswith('to ') and len(correct_answer.split()) > 5:
        return self._generate_sentence_structure_options(correct_answer, context_entities, subject)

    # For shorter answers, use enhanced entity substitution with better field matching
    else:
        return self._generate_field_appropriate_options(correct_answer, answer_entities, context_entities, subject)

def _generate_sentence_structure_options(self, correct_answer, context_entities, subject):
    """
    IMPROVED: Generate coherent sentence options with better structure
    """
    options = []
    answer_lower = correct_answer.lower()

    # Fix the main issue: "To X on a copy..." -> "To X a copy..."
    if ' on a copy of' in answer_lower:
        base_part = correct_answer.split(' on a copy')[0]
        ending_part = ' a copy' + correct_answer.split(' on a copy')[1]

        # Generate better verb alternatives for biology/science
        if subject in ['biology', 'science']:
            verbs = ['transmit', 'preserve', 'maintain', 'transfer']
            for verb in verbs[:3]:
                option = f"{base_part.replace('pass', verb)} {ending_part}"
                options.append(option)
                if len(options) >= 3:
                    break

    # Handle "To make X more organized..." patterns
    elif 'to make' in answer_lower and 'more organized' in answer_lower:
        if subject == 'computer_science':
            replacements = [
                ('code', 'programs'),
                ('code', 'software'),
                ('code', 'applications')
            ]

            for old, new in replacements:
                if old in answer_lower:
                    option = correct_answer.replace(old, new, 1)
                    options.append(option)
                    if len(options) >= 3:
                        break

    # Handle "as X" patterns better
    elif ' as ' in answer_lower:
        base_part = correct_answer.split(' as ')[0]

        # Better field-specific targets
        if subject == 'computer_science':
            cs_targets = ['interfaces', 'classes', 'parameters']
        elif subject in ['biology', 'science']:
            cs_targets = ['structures', 'components', 'systems']
        elif subject == 'history':
            cs_targets = ['institutions', 'movements', 'principles']
        elif subject == 'economics':
            cs_targets = ['systems', 'mechanisms', 'processes']
        else:
            cs_targets = ['components', 'elements', 'structures']

        for target in cs_targets:
            option = f"{base_part} as {target}"
            if option.lower() != answer_lower:
                options.append(option)
                if len(options) >= 3:
                    break

    return options[:3]

def _generate_field_appropriate_options(self, correct_answer, answer_entities, context_entities, subject):
    """
    NEW: Generate field-specific options with improved similarity and coherence
    """
    options = []

    # Single word or short phrase answers
    if len(correct_answer.split()) <= 3:
        return self._generate_similar_field_terms(correct_answer, context_entities, subject)

    # Medium length answers - use intelligent substitution
    else:
        return self._generate_structured_field_options(correct_answer, answer_entities, context_entities, subject)

def _generate_similar_field_terms(self, correct_answer, context_entities, subject):
    """
    ENHANCED: Prevent repetition with smart session management
    """
    options = []
    correct_lower = correct_answer.lower().strip()

    # Force economics subject detection
    if any(term in correct_lower for term in ['economic', 'cost', 'revenue', 'competition', 'supply', 'demand', 'market']):
        subject = 'economics'

    # Smart session management - reset if needed
    self._smart_session_management(correct_answer, subject)

    # Initialize session tracking if not exists
    if not hasattr(self, '_used_terms_session'):
        self._used_terms_session = set()

    if subject == 'computer_science':
        cs_term_categories = {
            'data_structures': ['Set', 'List', 'Map', 'Array', 'Collection'],
            'interfaces': ['Interface', 'Abstract class', 'Implementation'],
            'execution': ['Thread', 'Process', 'Task'],
            'concepts': ['Class', 'Method', 'Function']
        }

        for category, terms in cs_term_categories.items():
            if any(term.lower() in correct_lower for term in terms):
                for term in terms:
                    if (term.lower() != correct_lower and
                        term not in self._used_terms_session):
                        options.append(term)
                        self._used_terms_session.add(term)
                        if len(options) >= 3:
                            break
                break

    elif subject == 'economics':
        econ_term_categories = {
            'growth_types': ['Sustainable financial growth and development',
                           'Balanced economic growth and development',
                           'Rapid economic growth and development'],
            'market_types': ['Perfect competition', 'Oligopoly', 'Monopoly'],
            'costs': ['Total cost', 'Average cost', 'Fixed cost'],
            'revenues': ['Marginal revenue', 'Average revenue', 'Net revenue'],
            'deficits': ['Trade deficit', 'Fiscal deficit', 'Current deficit'],
            'policies': ['Monetary policy', 'Trade policy', 'Tax policy']
        }

        # Find best matching category
        best_category = None
        for category, terms in econ_term_categories.items():
            if any(term.lower() in correct_lower for term in terms):
                best_category = category
                break

        if best_category:
            terms = econ_term_categories[best_category]
            for term in terms:
                if (term.lower() != correct_lower and
                    term not in self._used_terms_session):
                    options.append(term)
                    self._used_terms_session.add(term)
                    if len(options) >= 3:
                        break

        # If still not enough, use cross-category terms
        if len(options) < 3:
            fallback_terms = ['Market economy', 'Economic system', 'Trade balance']
            for term in fallback_terms:
                if (term.lower() != correct_lower and
                    term not in options and
                    term not in self._used_terms_session):
                    options.append(term)
                    self._used_terms_session.add(term)
                    if len(options) >= 3:
                        break

    elif subject in ['biology', 'science']:
        # ENHANCED: Much larger, more diverse biology term pools with rotation
        import random

        bio_term_categories = {
            'cellular': ['Cytoplasm', 'Vacuole', 'Endoplasmic reticulum', 'Golgi apparatus', 'Ribosome', 'Mitochondria'],
            'processes': ['Transcription', 'Translation', 'Glycolysis', 'Krebs cycle', 'Fermentation', 'Osmosis'],
            'structures': ['Peptide', 'Polysaccharide', 'Nucleotide', 'Amino acid', 'Fatty acid', 'Steroid'],
            'systems': ['Biosphere', 'Community', 'Habitat', 'Niche', 'Food chain', 'Food web'],
            'molecules': ['Hemoglobin', 'Insulin', 'Collagen', 'Keratin', 'Myosin', 'Actin'],
            'functions': ['Circulation', 'Digestion', 'Secretion', 'Absorption', 'Filtration', 'Immunity'],
            'genetics': ['Allele', 'Genotype', 'Phenotype', 'Mutation', 'Crossing over', 'Linkage'],
            'development': ['Embryogenesis', 'Differentiation', 'Morphogenesis', 'Apoptosis', 'Regeneration', 'Growth'],
            'ecology': ['Symbiosis', 'Parasitism', 'Mutualism', 'Commensalism', 'Predation', 'Competition']
        }

        # Try specific matching first with unused terms
        matched = False
        for category, terms in bio_term_categories.items():
            if any(term.lower() in correct_lower for term in terms):
                available_terms = [t for t in terms
                                 if (t.lower() != correct_lower and
                                     t not in self._used_terms_session)]
                random.shuffle(available_terms)

                for term in available_terms[:3]:
                    options.append(term)
                    self._used_terms_session.add(term)
                    if len(options) >= 3:
                        matched = True
                        break
                if matched:
                    break

        # If no specific match, use diverse selection from ALL categories
        if not matched or len(options) < 3:
            # Create MUCH larger diverse pool from all categories
            all_bio_terms = []
            for category_terms in bio_term_categories.values():
                all_bio_terms.extend(category_terms)

            # Add even more diverse terms
            additional_terms = [
                'Chlorophyll', 'Carotenoid', 'Anthocyanin', 'Pectin', 'Cellulose', 'Lignin',
                'Tropism', 'Transpiration', 'Phototropism', 'Geotropism', 'Thigmotropism',
                'Homeostasis', 'Adaptation', 'Evolution', 'Natural selection', 'Speciation',
                'Biodiversity', 'Conservation', 'Sustainability', 'Biomagnification'
            ]
            all_bio_terms.extend(additional_terms)

            # Filter out used terms and shuffle
            available_pool = [term for term in all_bio_terms
                            if (term.lower() != correct_lower and
                                term not in self._used_terms_session)]
            random.shuffle(available_pool)

            for term in available_pool:
                options.append(term)
                self._used_terms_session.add(term)
                if len(options) >= 3:
                    break

    elif subject == 'history':
        hist_term_categories = {
            'movements': ['Renaissance', 'Reformation', 'Revolution', 'Enlightenment'],
            'institutions': ['Church', 'Empire', 'Kingdom', 'Republic'],
            'concepts': ['Christianity', 'Protestantism', 'Catholicism']
        }

        for category, terms in hist_term_categories.items():
            if any(term.lower() in correct_lower for term in terms):
                for term in terms:
                    if (term.lower() != correct_lower and
                        term not in self._used_terms_session):
                        options.append(term)
                        self._used_terms_session.add(term)
                        if len(options) >= 3:
                            break
                break

    return options[:3]


def _generate_structured_field_options(self, correct_answer, answer_entities, context_entities, subject):
    """
    NEW: Generate options for medium-length answers with field-appropriate structure
    """
    options = []

    # For economics questions with long technical terms
    if subject == 'economics' and len(correct_answer.split()) > 2:
        # Try to maintain the structure while changing key economic terms
        all_concepts = context_entities.get('concepts', [])

        # Economic structure patterns
        if 'economic' in correct_answer.lower():
            econ_alternatives = ['economic', 'financial', 'monetary', 'fiscal']
            for alt in econ_alternatives:
                if alt != 'economic':
                    option = correct_answer.replace('economic', alt, 1)
                    if option != correct_answer:
                        options.append(option)
                        if len(options) >= 3:
                            break

        # For "sustainable ... and ..." patterns
        elif ' and ' in correct_answer:
            parts = correct_answer.split(' and ')
            if len(parts) == 2:
                base = parts[0]
                econ_endings = ['policy', 'stability', 'efficiency', 'regulation']
                for ending in econ_endings:
                    if ending not in correct_answer.lower():
                        option = f"{base} and {ending}"
                        options.append(option)
                        if len(options) >= 3:
                            break

    # For history questions with long phrases
    elif subject == 'history' and len(correct_answer.split()) > 3:
        # Maintain historical context while changing specific terms
        hist_terms = {
            'Church': ['Empire', 'Kingdom', 'Republic'],
            'state': ['nation', 'country', 'empire'],
            'bankruptcy': ['crisis', 'collapse', 'decline'],
            'Revolution': ['Reformation', 'Renaissance', 'Movement']
        }

        for original, alternatives in hist_terms.items():
            if original.lower() in correct_answer.lower():
                for alt in alternatives:
                    option = correct_answer.replace(original, alt, 1)
                    if len(options) >= 3:
                        break
                if len(options) >= 3:
                    break

    # For biology medium phrases
    elif subject in ['biology', 'science'] and len(correct_answer.split()) > 2:
        bio_terms = {
            'cell': ['tissue', 'organ', 'organism'],
            'organism': ['system', 'structure', 'component'],
            'membrane': ['protein', 'enzyme', 'molecule']
        }

        for original, alternatives in bio_terms.items():
            if original.lower() in correct_answer.lower():
                for alt in alternatives:
                    option = correct_answer.replace(original, alt, 1)
                    if option != correct_answer:
                        options.append(option)
                        if len(options) >= 3:
                            break
                if len(options) >= 3:
                    break

    return options[:3]

def _get_field_appropriate_replacements(self, main_noun, all_concepts, subject):
    """
    NEW: Get field-appropriate replacements for a given noun
    """
    replacements = []

    field_mappings = {
        'computer_science': {
            'objects': ['classes', 'interfaces', 'components'],
            'functions': ['methods', 'procedures', 'algorithms'],
            'applications': ['systems', 'frameworks', 'programs']
        },
        'biology': {
            'organisms': ['systems', 'structures', 'components'],
            'cells': ['tissues', 'organs', 'organelles'],
            'processes': ['mechanisms', 'functions', 'systems']
        },
        'history': {
            'state': ['empire', 'kingdom', 'nation'],
            'Church': ['Empire', 'Kingdom', 'Institution'],
            'world': ['civilization', 'society', 'culture']
        },
        'economics': {
            'development': ['growth', 'progress', 'expansion'],
            'quantity': ['amount', 'measure', 'value'],
            'market': ['economy', 'system', 'sector']
        }
    }

    # Get mappings for the subject
    subject_mappings = field_mappings.get(subject, {})

    # Find appropriate replacements
    main_noun_lower = main_noun.lower()
    for key, alternatives in subject_mappings.items():
        if key in main_noun_lower or main_noun_lower in key:
            replacements.extend(alternatives)
            break

    # If no direct mapping, use context-based alternatives
    if not replacements:
        for concept in all_concepts:
            if (len(concept.split()) <= 2 and
                len(concept) >= 4 and
                concept.lower() != main_noun_lower):
                replacements.append(concept)
                if len(replacements) >= 5:
                    break

    return replacements[:3]



def _is_meaningful_substitute(self, substitute, original, subject):
    """
    ENHANCED: Better subject detection and validation
    """
    substitute_lower = substitute.lower()
    original_lower = original.lower()

    # Basic invalid substitutes
    if substitute_lower in ['the', 'this', 'that', 'these', 'those', 'and', 'or', 'but']:
        return False

    # CRITICAL: Force economics detection
    if any(term in original_lower for term in ['economic', 'cost', 'revenue', 'market', 'competition', 'deficit']):
        subject = 'economics'

    # CRITICAL: Prevent subject mixing
    if subject == 'economics':
        # Economics should NOT have CS terms
        cs_terms = ['interface', 'class', 'method', 'thread', 'list', 'set', 'map', 'array']
        if any(term in substitute_lower for term in cs_terms):
            return False

        # Should have economic relevance
        econ_terms = ['economic', 'market', 'cost', 'revenue', 'growth', 'development',
                     'competition', 'policy', 'deficit', 'trade', 'fiscal', 'sustainable']
        if any(term in original_lower for term in econ_terms):
            return any(term in substitute_lower for term in econ_terms) or len(substitute_lower) > 8

    elif subject == 'computer_science':
        # Reject obviously wrong terms for CS
        if substitute_lower in ['organisms', 'cells', 'tissues', 'empire', 'church']:
            return False

    elif subject in ['biology', 'science']:
        # Reject CS and other field terms
        if substitute_lower in ['interface', 'class', 'thread', 'empire', 'revolution']:
            return False

    elif subject == 'history':
        # Reject modern CS/tech terms
        if substitute_lower in ['interface', 'thread', 'class', 'cell', 'organism']:
            return False

    return len(substitute) >= 3 and not substitute_lower.isdigit()

def _is_too_similar_to_existing(self, new_option, existing_options):
    """
    Very lenient similarity check - only prevent exact duplicates
    """
    if not existing_options:
        return False

    for existing in existing_options:
        # Only prevent exact matches (case insensitive)
        if new_option.lower().strip() == existing.lower().strip():
            return True

    return False

def _is_grammatically_valid(self, text):
    """
    Much more lenient grammar validation
    """
    try:
        # Only check for obvious broken patterns
        broken_patterns = [
            r'\ba\s+a\s+',  # "a a something"
            r'\w+\s*\(\s*$',  # incomplete parentheses at end
        ]

        for pattern in broken_patterns:
            if re.search(pattern, text.lower()):
                return False

        # Very basic length check
        if len(text.split()) < 1:
            return False

        return True
    except:
        return True

def _substitute_entity_in_answer(self, answer, original_entity, new_entity):
    """
    Simple entity substitution with basic duplication check
    """
    # Try the substitution
    if original_entity in answer:
        result = answer.replace(original_entity, new_entity, 1)
    elif original_entity.lower() in answer.lower():
        words = answer.split()
        for i, word in enumerate(words):
            if word.lower() == original_entity.lower():
                words[i] = new_entity
                result = ' '.join(words)
                break
            elif original_entity.lower() in word.lower():
                words[i] = word.replace(original_entity.lower(), new_entity.lower())
                result = ' '.join(words)
                break
        else:
            return None
    else:
        return None

    # Only check for obvious consecutive duplications
    if re.search(r'\b(\w+)\s+\1\b', result.lower()):  # "word word" pattern
        return None

    return result

def _generate_semantic_similarity_options(self, correct_answer, context, doc_answer):
    """
    ENHANCED: Better semantic options with field-aware quality control
    """
    options = []

    # Force economics detection
    if any(term in correct_answer.lower() for term in ['economic', 'cost', 'revenue', 'market', 'competition', 'supply', 'demand']):
        subject = 'economics'
    else:
        subject = getattr(self, '_current_subject', 'general')

    # Get meaningful tokens only
    key_tokens = [token for token in doc_answer
                 if token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and
                 not token.is_stop and
                 len(token.text) > 3 and
                 not token.text.lower() in ['make', 'code', 'more', 'things', 'chapter']]

    if not key_tokens:
        return []

    # Get context tokens with better filtering
    doc_context = self.nlp(context)
    context_tokens = [token for token in doc_context
                     if token.pos_ in ['NOUN', 'PROPN', 'ADJ'] and
                     not token.is_stop and
                     len(token.text) > 3 and
                     not re.search(r'^\d+', token.text)]

    # CRITICAL: Better semantic filtering by subject
    if subject == 'economics':
        # Only use economically relevant words
        econ_words = ['financial', 'monetary', 'fiscal', 'market', 'trade', 'policy',
                     'development', 'growth', 'revenue', 'cost', 'competition']
        context_tokens = [token for token in context_tokens
                         if any(econ_word in token.text.lower() for econ_word in econ_words)]

    elif subject in ['biology', 'science']:
        # Only use biologically relevant words
        bio_words = ['cellular', 'genetic', 'molecular', 'biological', 'organic',
                    'living', 'life', 'organism', 'development', 'reproduction']
        context_tokens = [token for token in context_tokens
                         if any(bio_word in token.text.lower() for bio_word in bio_words)]

    # ENHANCED: Smarter substitution with meaning preservation
    used_tokens = set()
    for key_token in key_tokens[:2]:
        for context_token in context_tokens[:8]:  # Reduced from 10
            if (context_token.text.lower() != key_token.text.lower() and
                context_token.text not in used_tokens and
                len(context_token.text) >= 4 and
                self._is_semantically_appropriate(key_token.text, context_token.text, subject)):

                option = correct_answer.replace(key_token.text, context_token.text, 1)
                cleaned_option = self._clean_option_text(option)

                if (cleaned_option and
                    cleaned_option != correct_answer and
                    len(cleaned_option.split()) >= 2 and
                    self._is_grammatically_coherent(cleaned_option) and
                    not self._is_too_similar_to_existing(cleaned_option, options)):
                    options.append(cleaned_option)
                    used_tokens.add(context_token.text)
                    if len(options) >= 3:
                        return options[:3]

    return options

def _is_grammatically_coherent(self, text):
    """
    NEW: Check if the generated option makes grammatical sense
    """
    # Reject obviously broken patterns
    broken_patterns = [
        r'\b(to|the|a|an)\s+(things|chapter|theory)\s+of\b',  # "to things of", "the chapter of"
        r'\bpass on a (things|chapter|theory)\b',  # "pass on a things"
        r'\bexplain what (chapter|theory|things) and\b',  # "explain what chapter and"
        r'\b(things|chapter|theory) of the genetic\b',  # "things of the genetic"
    ]

    text_lower = text.lower()
    for pattern in broken_patterns:
        if re.search(pattern, text_lower):
            return False

    return True

def _is_semantically_appropriate(self, original_word, replacement_word, subject):
    """
    NEW: Check if replacement makes semantic sense
    """
    original_lower = original_word.lower()
    replacement_lower = replacement_word.lower()

    # Don't replace with totally unrelated words
    bad_replacements = {
        'demand': ['chapter', 'theory', 'introduction', 'lecture'],
        'supply': ['chapter', 'theory', 'section', 'notes'],
        'economic': ['things', 'living', 'reproduce', 'develop'],
        'copy': ['things', 'living', 'chapter', 'theory'],
        'genetic': ['things', 'chapter', 'theory', 'section']
    }

    if original_lower in bad_replacements:
        if replacement_lower in bad_replacements[original_lower]:
            return False

    # Subject-specific appropriateness
    if subject == 'economics':
        # Economics terms should be replaced with economics-related terms
        if original_lower in ['economic', 'market', 'cost', 'revenue']:
            economics_terms = ['financial', 'monetary', 'fiscal', 'trade', 'policy']
            return replacement_lower in economics_terms or 'economic' in replacement_lower

    elif subject in ['biology', 'science']:
        # Biology terms should be replaced with biology-related terms
        if original_lower in ['genetic', 'living', 'reproduce']:
            bio_terms = ['cellular', 'molecular', 'biological', 'organic', 'developmental']
            return replacement_lower in bio_terms or any(bio in replacement_lower for bio in bio_terms)

    return True

# Add a method to track current subject during processing
def _set_current_subject(self, subject):
    """Store current subject for cross-method access"""
    self._current_subject = subject

# Additional method to add diversity for biology specifically
def _add_biology_diversity(self, base_options, correct_answer):
    """
    Add more diverse biology options to prevent repetition
    """
    if len(base_options) >= 3:
        return base_options

    # Extended biology term pools
    bio_pools = {
        'structures': ['Nucleus', 'Membrane', 'Ribosome', 'Mitochondria', 'Chloroplast'],
        'processes': ['Photosynthesis', 'Respiration', 'Digestion', 'Circulation', 'Excretion'],
        'molecules': ['DNA', 'RNA', 'Protein', 'Enzyme', 'Lipid', 'Carbohydrate'],
        'levels': ['Molecule', 'Organelle', 'Cell', 'Tissue', 'Organ', 'System']
    }

    correct_lower = correct_answer.lower()
    additional_options = []

    # Smart selection based on context
    for pool_name, terms in bio_pools.items():
        for term in terms:
            if (term.lower() != correct_lower and
                term not in base_options and
                term not in additional_options):
                additional_options.append(term)
                if len(base_options) + len(additional_options) >= 3:
                    break
        if len(base_options) + len(additional_options) >= 3:
            break

    return (base_options + additional_options)[:3]

def _generate_context_based_options(self, correct_answer, context, doc_context):
    """
    ENHANCED: Better context extraction with improved relevance
    """
    options = []

    # Extract meaningful phrases from context
    candidates = []

    # Get noun phrases with better filtering
    for chunk in doc_context.noun_chunks:
        phrase = chunk.text.strip()
        if (2 <= len(phrase.split()) <= 6 and  # Slightly longer phrases allowed
            phrase.lower() != correct_answer.lower() and
            len(phrase) >= 5 and  # Minimum meaningful length
            not re.search(r'^\d+', phrase) and
            not phrase.lower().startswith(('a the', 'the the'))):
            candidates.append(phrase)

    # Get individual meaningful terms as fallback
    for token in doc_context:
        if (token.pos_ in ['NOUN', 'PROPN'] and
            not token.is_stop and
            len(token.text) >= 4 and
            token.text.lower() != correct_answer.lower() and
            not re.search(r'^\d+', token.text)):
            candidates.append(token.text)

    # Score candidates for relevance
    answer_doc = self.nlp(correct_answer)
    answer_tokens = set(token.text.lower() for token in answer_doc if not token.is_stop)

    scored_candidates = []
    for candidate in candidates:
        candidate_doc = self.nlp(candidate)
        candidate_tokens = set(token.text.lower() for token in candidate_doc if not token.is_stop)

        # Calculate relevance
        overlap = answer_tokens & candidate_tokens
        if len(overlap) > 0 and len(overlap) < len(answer_tokens):
            score = len(overlap) / max(len(answer_tokens), len(candidate_tokens))
            if 0.1 <= score <= 0.8:  # Broader range for relevance
                scored_candidates.append((candidate, score))

    # Sort and select diverse options
    scored_candidates.sort(key=lambda x: x[1], reverse=True)

    for candidate, score in scored_candidates:
        cleaned_candidate = self._clean_option_text(candidate)
        if (cleaned_candidate and
            not self._is_too_similar_to_existing(cleaned_candidate, options) and
            len(cleaned_candidate.split()) >= 1):  # More lenient
            options.append(cleaned_candidate)
            if len(options) >= 3:
                return options[:3]

    return options

def _finalize_options(self, correct_answer, distractors):
    """
    Create final options dictionary with proper keys and randomization
    """
    import random

    # Clean the correct answer
    correct_answer = self._clean_option_text(correct_answer)

    # Create all options list
    all_options = [correct_answer] + distractors

    # Shuffle to randomize position of correct answer
    option_pairs = list(enumerate(all_options))
    random.shuffle(option_pairs)

    # Create options dictionary
    options = {}
    correct_key = None
    keys = ['A', 'B', 'C', 'D']

    for i, (original_index, option_text) in enumerate(option_pairs):
        key = keys[i]
        options[key] = option_text
        if original_index == 0:  # This was the correct answer
            correct_key = key

    return options, correct_key

def _clean_option_text(self, option):
    """
    ENHANCED: Better cleaning with specific pattern fixes
    """
    if not option:
        return ""

    # Basic cleaning
    option = re.sub(r'\s+', ' ', option).strip()

    # CRITICAL FIXES for economics and biology issues:

    # Fix broken economics patterns
    option = re.sub(r'\bTo explain what (Chapter|Theory|Supply) and supply\b',
                   'To explain what demand and supply', option, flags=re.IGNORECASE)
    option = re.sub(r'\bTo explain what demand and (Chapter|Theory)\b',
                   'To explain what demand and supply', option, flags=re.IGNORECASE)

    # Fix broken biology patterns
    option = re.sub(r'\bPass on a (Things|Reproduce|Develop) of the genetic\b',
                   'Pass on the genetic information to the', option, flags=re.IGNORECASE)
    option = re.sub(r'\bTo (Things|Living|Reproduce) a copy\b',
                   'To pass a copy', option, flags=re.IGNORECASE)

    # Fix general broken patterns
    option = re.sub(r'\b(the|a|an) (things|chapter|theory) of\b',
                   'the process of', option, flags=re.IGNORECASE)

    # Remove obvious artifacts and duplications
    option = re.sub(r'\b(the|a|an)\s+(the|a|an)\b', r'\1', option, flags=re.IGNORECASE)
    option = re.sub(r'\b(\w+)\s+\1\b', r'\1', option, flags=re.IGNORECASE)

    # Smart capitalization
    if option:
        if option.lower().startswith('to '):
            option = 'To ' + option[3:]
        elif option.lower().startswith('the '):
            option = 'The ' + option[4:]
        elif not option[0].isupper():
            option = option[0].upper() + option[1:] if len(option) > 1 else option.upper()

    return option.strip()

def _is_valid_option(self, option, correct_answer):
    """
    MUCH MORE RELAXED: Allow more questions to generate by being very lenient
    """
    if not option or len(option.strip()) < 1:  # Reduced from 2 to 1
        return False

    # Must be different from correct answer
    if option.lower().strip() == correct_answer.lower().strip():
        return False

    # Very generous length check
    word_count = len(option.split())
    if word_count < 1 or word_count > 30:  # Increased from 25 to 30
        return False

    option_lower = option.lower().strip()

    # Only reject VERY obvious nonsense - be much more permissive
    severe_nonsense_patterns = [
        r'^\d+\.\d+\.\d+',  # Multiple decimals like "6.2.3"
        r'^[^a-zA-Z]*$',    # No letters at all
        r'^\w{1}$',         # Only single letters (reduced from 1-2 to just 1)
    ]

    for pattern in severe_nonsense_patterns:
        if re.search(pattern, option_lower):
            return False

    # Much more relaxed content check
    content = re.sub(r'\b(the|a|an|of|in|to|for|with|and|or|as|is|are|be|been|being)\b', '', option_lower)
    content = re.sub(r'\s+', '', content)

    # Only reject if there's literally no meaningful content
    if len(content) < 1:  # Reduced from 2 to 1
        return False

    # Allow almost everything else - even if it looks odd, let it through
    return True

def _is_complete_phrase(self, text):
    """
    Check if the text forms a complete, meaningful phrase using spaCy
    """
    doc = self.nlp(text)

    # Must have at least one noun or proper noun
    has_noun = any(token.pos_ in ['NOUN', 'PROPN'] for token in doc)

    # Must not end with incomplete grammar
    if len(doc) > 0:
        last_token = doc[-1]
        if last_token.pos_ in ['DET', 'PREP', 'CONJ']:  # Determiners, prepositions, conjunctions
            return False

    # Check for basic grammatical completeness
    has_content_word = any(token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'VERB'] for token in doc)

    return has_noun and has_content_word

def reset_session_tracking(self):
    """Reset session tracking for new document processing"""
    if hasattr(self, '_used_terms_session'):
        self._used_terms_session.clear()

def _smart_session_management(self, correct_answer, subject):
    """
    Smart session management - reset if we're running low on options
    """
    if not hasattr(self, '_used_terms_session'):
        self._used_terms_session = set()
        return

    # Get total available terms for current subject
    total_available = 0
    if subject in ['biology', 'science']:
        # Count all biology terms from the expanded pools
        bio_categories = {
            'cellular': 6, 'processes': 6, 'structures': 6, 'systems': 6,
            'molecules': 6, 'functions': 6, 'genetics': 6, 'development': 6,
            'ecology': 6, 'additional': 19  # From additional_terms
        }
        total_available = sum(bio_categories.values())
    elif subject == 'economics':
        total_available = 21  # All econ terms
    elif subject == 'computer_science':
        total_available = 17  # All CS terms
    elif subject == 'history':
        total_available = 11  # All history terms

    # If we've used more than 70% of available terms, reset to allow diversity
    used_count = len(self._used_terms_session)
    if total_available > 0 and (used_count / total_available) > 0.7:
        self._used_terms_session.clear()

MultiTaskEducationalModel._generate_contextual_options = _generate_contextual_options
MultiTaskEducationalModel._apply_sentence_case = _apply_sentence_case
MultiTaskEducationalModel._apply_sentence_case_to_all_options = _apply_sentence_case_to_all_options
MultiTaskEducationalModel._extract_key_entities = _extract_key_entities
MultiTaskEducationalModel._generate_entity_based_options = _generate_entity_based_options
MultiTaskEducationalModel._generate_sentence_structure_options = _generate_sentence_structure_options
MultiTaskEducationalModel._generate_field_appropriate_options = _generate_field_appropriate_options
MultiTaskEducationalModel._generate_similar_field_terms = _generate_similar_field_terms
MultiTaskEducationalModel._generate_structured_field_options = _generate_structured_field_options
MultiTaskEducationalModel._get_field_appropriate_replacements = _get_field_appropriate_replacements
MultiTaskEducationalModel._is_meaningful_substitute = _is_meaningful_substitute
MultiTaskEducationalModel._is_grammatically_valid = _is_grammatically_valid
MultiTaskEducationalModel._is_too_similar_to_existing = _is_too_similar_to_existing
MultiTaskEducationalModel._substitute_entity_in_answer = _substitute_entity_in_answer
MultiTaskEducationalModel._generate_semantic_similarity_options = _generate_semantic_similarity_options
MultiTaskEducationalModel._is_semantically_appropriate = _is_semantically_appropriate
MultiTaskEducationalModel._is_grammatically_coherent = _is_grammatically_coherent
MultiTaskEducationalModel._set_current_subject = _set_current_subject
MultiTaskEducationalModel._add_biology_diversity = _add_biology_diversity
MultiTaskEducationalModel._generate_context_based_options = _generate_context_based_options
MultiTaskEducationalModel._finalize_options = _finalize_options
MultiTaskEducationalModel._clean_option_text = _clean_option_text
MultiTaskEducationalModel._is_valid_option = _is_valid_option
MultiTaskEducationalModel._is_complete_phrase = _is_complete_phrase
MultiTaskEducationalModel.reset_session_tracking = reset_session_tracking
MultiTaskEducationalModel._smart_session_management = _smart_session_management

def _validate_final_mcq(self, mcq):
    """
    Very lenient final validation
    """
    if not mcq or not mcq.get('question') or not mcq.get('options') or not mcq.get('correct_answer'):
        return False

    question = mcq['question']
    options = mcq['options']
    correct_key = mcq['correct_answer']

    # Basic checks only
    if not question.strip().endswith('?') or len(question.split()) < 3:
        return False

    if len(options) < 2 or correct_key not in options:
        return False

    # Only prevent exact duplicate options
    option_texts = [opt.lower().strip() for opt in options.values()]
    if len(set(option_texts)) != len(option_texts):
        return False

    return True

def _format_mcq_final(self, mcq):
    """
    Consistent final formatting: trimmed, capitalized.
    """
    question = self._shorten_sentence(mcq['question'], 15)
    if not question.endswith('?'):
        question += '?'
    formatted_options = {}
    for key, option in mcq['options'].items():
        formatted_options[key] = self._shorten_sentence(option, 8)
    return {
        "question": question,
        "options": formatted_options,
        "correct_answer": mcq['correct_answer']
    }

def _create_fallback_mcq(self, subject):
    """
    ORIGINAL WORKING fallback
    """
    fallback_questions = {
        "mathematics": {
            "question": "What is a fundamental concept in mathematical analysis?",
            "options": {
                "A": "Function",
                "B": "Random calculation",
                "C": "Memorization",
                "D": "Theory only"
            },
            "correct_answer": "A"
        },
        "science": {
            "question": "What characterizes scientific methodology?",
            "options": {
                "A": "Observation and evidence",
                "B": "Acceptance without testing",
                "C": "Personal opinions",
                "D": "Rejection of theories"
            },
            "correct_answer": "A"
        },
        "computer_science": {
            "question": "What is a key principle in programming?",
            "options": {
                "A": "Systematic design",
                "B": "Random coding",
                "C": "Ignoring efficiency",
                "D": "Avoiding documentation"
            },
            "correct_answer": "A"
        },
        "biology": {
            "question": "What is fundamental to biological understanding?",
            "options": {
                "A": "Study of living organisms",
                "B": "Memorizing names",
                "C": "Focusing only on cells",
                "D": "Ignoring environment"
            },
            "correct_answer": "A"
        },
        "chemistry": {
            "question": "What characterizes chemical understanding?",
            "options": {
                "A": "Atomic interactions",
                "B": "Memorizing formulas",
                "C": "Studying in isolation",
                "D": "Ignoring energy"
            },
            "correct_answer": "A"
        },
        "physics": {
            "question": "What is fundamental to physics?",
            "options": {
                "A": "Natural phenomena principles",
                "B": "Memorizing equations",
                "C": "Studying without effects",
                "D": "Ignoring math"
            },
            "correct_answer": "A"
        },
        "general": {
            "question": "What promotes effective learning?",
            "options": {
                "A": "Active engagement",
                "B": "Passive memorization",
                "C": "Avoiding challenges",
                "D": "Studying alone"
            },
            "correct_answer": "A"
        }
    }

    return fallback_questions.get(subject, fallback_questions["general"])

MultiTaskEducationalModel._validate_final_mcq = _validate_final_mcq
MultiTaskEducationalModel._format_mcq_final = _format_mcq_final
MultiTaskEducationalModel._create_fallback_mcq = _create_fallback_mcq

def _validate_qa_pair(self, qa_pair):
    """
    Validate that the question and answer make sense together
    """
    if not qa_pair or not qa_pair.get('question') or not qa_pair.get('answer'):
        return False

    question = qa_pair['question']
    answer = qa_pair['answer']

    # Question validation
    if (not question.strip().endswith('?') or
        len(question.split()) < 4 or
        len(question.split()) > 15 or
        any(artifact in question for artifact in ['', '---', '**'])):
        return False

    # Answer validation
    if (len(answer.split()) < 3 or
        len(answer.split()) > 25 or
        answer.lower().startswith(('this', 'that', 'it', 'they', 'such')) or
        any(artifact in answer for artifact in ['', '---', '**'])):
        return False

    # Logical consistency
    if qa_pair.get('concept'):
        concept = qa_pair['concept'].lower()
        # Concept should appear in question but not dominate the answer
        if concept not in question.lower():
            return False
        if answer.lower().count(concept) > 1:
            return False

    return True

def _randomize_mcq_options(self, mcq_data):
    """Randomize MCQ options and update correct answer"""
    options = mcq_data["options"]
    correct_answer = mcq_data["correct_answer"]

    # Get the correct answer text
    correct_text = options[correct_answer]

    # Create list of all options with their texts
    option_items = [(key, text) for key, text in options.items()]

    # Shuffle the options
    random.shuffle(option_items)

    # Create new options dict and find new correct answer key
    new_options = {}
    new_correct_key = None

    for i, (old_key, text) in enumerate(option_items):
        new_key = chr(ord('A') + i)
        new_options[new_key] = text
        if text == correct_text:
            new_correct_key = new_key

    return {
        "question": mcq_data["question"],
        "options": new_options,
        "correct_answer": new_correct_key
    }

def _contains_subject_terms(self, sentence, subject):
    """
    Check if sentence contains subject-relevant terminology
    """
    subject_vocabularies = {
        'computer_science': [
            'class', 'method', 'function', 'algorithm', 'data', 'structure', 'object',
            'interface', 'system', 'programming', 'code', 'software', 'array',
            'list', 'exception', 'thread', 'lambda', 'generic', 'collection'
        ],
        'biology': [
            'cell', 'organism', 'DNA', 'protein', 'tissue', 'organ', 'reproduction',
            'genetic', 'biological', 'chromosome', 'metabolism', 'homeostasis'
        ],
        'history': [
            'empire', 'emperor', 'republic', 'democracy', 'revolution', 'civilization',
            'political', 'social', 'cultural', 'economic', 'military', 'religious'
        ],
        'general': [
            'concept', 'principle', 'theory', 'method', 'approach', 'process',
            'system', 'structure', 'function', 'purpose', 'role', 'importance'
        ]
    }

    relevant_terms = subject_vocabularies.get(subject, subject_vocabularies['general'])
    sentence_lower = sentence.lower()
    return any(term in sentence_lower for term in relevant_terms)

MultiTaskEducationalModel._validate_qa_pair = _validate_qa_pair
MultiTaskEducationalModel._randomize_mcq_options = _randomize_mcq_options
MultiTaskEducationalModel._contains_subject_terms = _contains_subject_terms

"""Process Pdf Files"""

def run_pdf_inference(model, processor, pdf_path, save_feedback=False):
    """
    Enhanced PDF inference: Generates summary and high-quality MCQs from educational PDF.
    """
    pdf_data = processor.process_educational_pdf(pdf_path)
    segments = pdf_data["segments"]

    print(f"Processing PDF: {pdf_path}")
    print(f"Found {len(segments)} segments")

    # =================== SUMMARY SEGMENT SELECTION ===================
    summary_segments = []
    for seg in segments:
        if (
            len(seg.split()) >= 30 and  # Inclusive
            len(sent_tokenize(seg)) >= 2 and
            (
                any(pattern in seg.lower() for pattern in [
                    # Programming patterns
                    'is defined as', 'refers to', 'means that', 'is used to', 'allows', 'enables',
                    'provides', 'creates', 'implements', 'manages', 'controls', 'consists of',
                    'contains', 'includes', 'features', 'has the ability', 'can be used',
                    'is responsible for', 'handles', 'processes', 'executes',
                    # Universal academic patterns
                    'function', 'process', 'system', 'structure', 'method', 'principle',
                    'theory', 'concept', 'analysis', 'study', 'research', 'evidence',
                    # Biology patterns
                    'cell', 'organism', 'DNA', 'protein', 'tissue', 'organ', 'enzyme',
                    'metabolism', 'photosynthesis', 'respiration', 'evolution', 'species',
                    'biological', 'living', 'life', 'genetic', 'molecular', 'cellular',
                    # History patterns
                    'empire', 'civilization', 'dynasty', 'revolution', 'war', 'battle',
                    'democracy', 'republic', 'monarchy', 'feudalism', 'renaissance',
                    'government', 'political', 'social', 'economic', 'cultural',
                    'historical', 'period', 'century', 'era', 'development', 'change',
                    'ancient', 'medieval', 'king', 'queen', 'church', 'religious',
                    'trade', 'commerce', 'people', 'population', 'power', 'influence',
                    # Economics
                    'market', 'supply', 'demand', 'economy', 'policy', 'price', 'cost',
                    'profit', 'investment', 'business', 'financial',
                    # Psychology
                    'behavior', 'cognitive', 'memory', 'brain', 'psychological',
                    # Other
                    'important', 'significant', 'essential', 'fundamental', 'critical',
                    'established', 'developed', 'emerged', 'occurred', 'resulted'
                ]) or re.search(r'[=<>≤≥∈⊆∪∩∀∃⇒⇔±×÷∞]', seg)
            ) and
            not seg.strip().lower().startswith((
                'this', 'that', 'these', 'those', 'it', 'they', 'such', 'for this', 'to avoid',
                'therefore', 'thus', 'hence', 'as a result', 'anyway', 'also', 'additionally'
            ))
        ):
            summary_segments.append(seg)

    print(f"Selected {len(summary_segments)} high-quality segments for summary")

    # ================ SUMMARY GENERATION LOGIC (unchanged) ================
    subject = pdf_data["subject"]
    all_bullet_points = []

    for seg in summary_segments:
        segment_key_points = model.extract_key_points(seg, subject)
        for point in segment_key_points:
            if (
                point and
                len(point) > 30 and
                len(point.split()) >= 8 and
                not point.endswith('?') and
                not point.lower().startswith((
                    'this is because', 'to avoid this', 'for this reason', 'therefore', 'thus',
                    'hence', 'as a result', 'consequently', 'in addition', 'furthermore', 'moreover',
                    'however', 'it is', 'it can be', 'they are', 'these are', 'such', 'anyway',
                    'also', 'additionally', 'this result', 'the result'
                )) and
                point not in all_bullet_points and
                not point.endswith((',', ';', 'and', 'or', 'but', ':'))
            ):
                is_valid_point = (
                    re.search(
                        r'\b(provides?|enables?|allows?|implements?|manages?|controls?|contains?|includes?|consists|features?|has|have|can|will|must|should|represents?|constitutes?|forms?|occurred?|happened?|emerged?|developed?|established?)\b.*\b(for|to|by|through|using|with|in|on|at|of|from|as|that|which|when|where|during)\b',
                        point, re.IGNORECASE
                    ) or
                    re.search(r'[=<>≤≥∈⊆∪∩∀∃⇒⇔±×÷∞]', point) or
                    re.search(
                        r'\b(characterized\s+by|distinguished\s+by|known\s+for|recognized\s+as|considered\s+to\s+be|defined\s+as|referred\s+to\s+as)\b',
                        point, re.IGNORECASE
                    )
                )

                subject_validation = (
                    re.search(
                        r'\b[A-Z][a-zA-Z]+\b.*\b(is|are|was|were|provides?|enables?|allows?|implements?|supports?|handles?|processes?|executes?|manages?|controls?|occurs?|happens?|results?|emerged|developed|functions?|contains?|includes?|represents?)\b',
                        point
                    ) or
                    re.search(r'[=<>≤≥∈⊆∪∩∀∃⇒⇔±×÷∞]', point)
                )

                starts_properly = re.match(r'^[A-Z][a-zA-Z]+', point)

                if is_valid_point and subject_validation and starts_properly:
                    all_bullet_points.append(point)

    TARGET_POINTS = 10
    if len(all_bullet_points) < TARGET_POINTS:
        print(f"Only found {len(all_bullet_points)} points, trying with relaxed criteria...")
        # You can add fallback/fuzzy logic here if needed

    if all_bullet_points:
        if len(all_bullet_points) > TARGET_POINTS:
            try:
                embeddings = model.embedder.encode(
                    all_bullet_points, convert_to_numpy=True, normalize_embeddings=True
                )
                similarity_matrix = np.dot(embeddings, embeddings.T)
                selected_indices = [0]
                for _ in range(min(TARGET_POINTS-1, len(all_bullet_points)-1)):
                    if not selected_indices:
                        break
                    avg_sim = np.mean([similarity_matrix[i, :] for i in selected_indices], axis=0)
                    candidates = [i for i in range(len(all_bullet_points)) if i not in selected_indices]
                    if not candidates:
                        break
                    next_idx = min(candidates, key=lambda i: avg_sim[i])
                    if avg_sim[next_idx] < 0.75:
                        selected_indices.append(next_idx)
                all_bullet_points = [all_bullet_points[i] for i in selected_indices]
            except Exception as e:
                all_bullet_points = all_bullet_points[:TARGET_POINTS]

        final_summary = "SUMMARY:\n"
        for point in all_bullet_points:
            clean_point = point.strip()
            if clean_point and clean_point[0].isalpha():
                clean_point = clean_point[0].upper() + clean_point[1:]
            if not clean_point.endswith(('.', '!', '?')):
                clean_point += '.'
            final_summary += f"- {clean_point}\n"

        final_summary = model._validate_summary(final_summary)
    else:
        final_summary = "SUMMARY:\n- No meaningful key points could be extracted from this document.\n"

    print(f"Summary generated successfully with {len(all_bullet_points)} key points")

    # ==================== MCQ GENERATION =======================
    print("Generating 5 high-quality multiple-choice questions...")
    mcq_pairs = []

    # ----------- PREMIUM SEGMENT FILTERING FOR MCQs -----------
    print("Pre-filtering segments for MCQ generation quality...")

    premium_segments = []

    for seg in segments:
        if (
            len(seg.split()) >= 50 and
            len(sent_tokenize(seg)) >= 3 and
            model._contains_subject_terms(seg, subject) and
            any(pattern in seg.lower() for pattern in [
                'is defined as', 'refers to', 'means that', 'is called', 'is known as',
                'is a type of', 'is a method', 'is a process', 'is used to', 'allows',
                'enables', 'provides', 'function of', 'purpose of', 'advantage of',
                'characterizes', 'distinguishes', 'difference between', 'compared to'
            ]) and
            not any(bad_pattern in seg.lower() for bad_pattern in [
                'for example', 'such as', 'in this case', 'as shown', 'above example',
                'we can see', 'let us', 'consider the', 'look at the'
            ])
        ):
            premium_segments.append(seg)

    print(f"Found {len(premium_segments)} premium segments for MCQ generation")

    # Fallback to good quality ones if not enough
    if len(premium_segments) < 20:
        for seg in segments:
            if (
                seg not in premium_segments and
                len(seg.split()) >= 35 and
                len(sent_tokenize(seg)) >= 2 and
                model._contains_subject_terms(seg, subject)
            ):
                premium_segments.append(seg)
                if len(premium_segments) >= 30:
                    break

    print(f"Using {len(premium_segments)} total segments for MCQ generation")

    # ------------- MCQ SCORING AND SELECTION -------------
    TARGET_MCQS = 5
    max_attempts_per_round = 10
    max_rounds = 3

    scored_segments = []
    for seg in premium_segments:
        score = 0
        seg_lower = seg.lower()
        if any(pattern in seg_lower for pattern in ['is defined as', 'refers to', 'means that']): score += 3
        if any(pattern in seg_lower for pattern in ['function', 'purpose', 'used to', 'allows', 'enables']): score += 2
        if any(pattern in seg_lower for pattern in ['difference between', 'compared to', 'advantage']): score += 2
        if len(seg.split()) > 60: score += 1
        if model._contains_subject_terms(seg, subject): score += 1
        scored_segments.append((score, seg))

    scored_segments.sort(key=lambda x: x[0], reverse=True)
    best_segments = [seg for score, seg in scored_segments[:25]]

    # ========== Only these two log lines are preserved ==========
    for round_num in range(max_rounds):
        if len(mcq_pairs) >= TARGET_MCQS:
            break

        start_idx = round_num * max_attempts_per_round
        end_idx = min(start_idx + max_attempts_per_round, len(best_segments))

        for i, seg in enumerate(best_segments[start_idx:end_idx]):
            if len(mcq_pairs) >= TARGET_MCQS:
                break

            print(f"Generating MCQ {len(mcq_pairs) + 1}/{TARGET_MCQS} ...")

            try:
                mcq_data = model.generate_mcq_from_content(
                    context=seg,
                    subject=subject,
                    key_entities=pdf_data.get("key_entities", []),
                    focus_concept=None
                )

                if (
                    mcq_data and
                    model._validate_final_mcq(mcq_data) and
                    model._is_good_question_type(mcq_data['question']) and
                    model._is_content_relevant(mcq_data, seg, subject)
                ):
                    mcq_result = {
                        "context": seg[:200] + "..." if len(seg) > 200 else seg,
                        "qa_data": mcq_data,
                        "subject": subject
                    }
                    mcq_pairs.append(mcq_result)
                    print(f"✓ Generated high-quality MCQ #{len(mcq_pairs)}")
            except Exception as e:
                continue

    print(f"MCQ generation completed: {len(mcq_pairs)} high-quality questions generated")

    return {
        "pdf_path": pdf_path,
        "subject": subject,
        "summary": final_summary,
        "qa_pairs": mcq_pairs,
    }

"""The Main Method"""

def main():
    """
    Main function with improved formatting and enhanced datasets - FULLY FIXED VERSION
    """
    print("Educational PDF Model - Starting Version")

    # Set configuration parameters
    MODEL_NAME = "google/flan-t5-base"    # Base model to use
    DRIVE_DIR = "/content/drive/MyDrive"   # Base Google Drive directory
    OUTPUT_DIR = f"{DRIVE_DIR}/model_output"
    CHECKPOINT_DIR = f"{OUTPUT_DIR}/checkpoints"
    DATA_DIR = f"{OUTPUT_DIR}/data"
    FINAL_MODEL_PATH = f"{OUTPUT_DIR}/final_model.pth"
    MAX_SAMPLES = 25000  # INCREASED sample size
    EPOCHS = 3
    BATCH_SIZE = 2
    SAVE_STEPS = 1000
    PDF_PATH = f"{DRIVE_DIR}/Advanced Java Lecture Notes 1.5.0.pdf"
    # PDF_PATH = f"{DRIVE_DIR}/HB(2).pdf"
    # PDF_PATH = f"{DRIVE_DIR}/macro1.pdf"
    # PDF_PATH = f"{DRIVE_DIR}/AP-World-History.pdf"

    # Create directories
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(DATA_DIR, exist_ok=True)

    # Check if model was already trained and saved
    if os.path.exists(FINAL_MODEL_PATH):
        print(f"Final model already exists at {FINAL_MODEL_PATH}. Loading model...")

        # Load the model and processor with minimal logging
        model = MultiTaskEducationalModel()
        model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location='cuda' if torch.cuda.is_available() else 'cpu'))
        model.tokenizer = AutoTokenizer.from_pretrained(f"{OUTPUT_DIR}/tokenizer")

        # Load processor
        import pickle
        with open(f"{OUTPUT_DIR}/pdf_processor.pkl", 'rb') as f:
            processor = pickle.load(f)

        print("Model and processor loaded!")

    else:
        print("No trained model found. Starting training process...")

        print("==== Loading and preparing datasets ====")
        try:
            # FIXED: Always regenerate datasets to ensure we have data
            print("Combining datasets...")
            # FIXED: Use the enhanced dataset function with correct format
            combined_df = combine_educational_datasets(max_samples=MAX_SAMPLES, shuffle=True)


            # FIXED: Proper task filtering for new format
            summ = combined_df[combined_df['task'] == 'summarization']
            mcq_temp = combined_df[combined_df['task'] == 'mcq_generation']  # FIXED: Use correct task name

            print(f"Before balancing:")
            print(f"- Summarization: {len(summ)} samples")
            print(f"- MCQ Generation: {len(mcq_temp)} samples")

            # Balance datasets if we have both types
            if len(summ) > 0 and len(mcq_temp) > 0:
                n_bal = min(len(summ), len(mcq_temp))
                summ = summ.sample(n_bal, random_state=42) if len(summ) > n_bal else summ
                mcq_temp = mcq_temp.sample(n_bal, random_state=42) if len(mcq_temp) > n_bal else mcq_temp
                combined_df = pd.concat([summ, mcq_temp]).sample(frac=1, random_state=42).reset_index(drop=True)
            elif len(mcq_temp) > 0:
                # If we only have MCQ data, use that
                combined_df = mcq_temp.sample(frac=1, random_state=42).reset_index(drop=True)
                print("Using only MCQ data as no summarization data is available")
            elif len(summ) > 0:
                # If we only have summarization data, use that
                combined_df = summ.sample(frac=1, random_state=42).reset_index(drop=True)
                print("Using only summarization data as no MCQ data is available")
            else:
                print("ERROR: No valid data after filtering!")
                return None, None

            # Save the properly formatted dataset
            combined_df.to_csv(f"{DATA_DIR}/training_data.csv", index=False)
            print(f"Combined dataset saved to {DATA_DIR}/training_data.csv")

            print(f"Dataset loaded with {len(combined_df)} samples")

            # FIXED: Use the new prepare_data method with combined_df
            print(f"FINAL DATASET BREAKDOWN:")
            print(f"- Summarization: {len(combined_df[combined_df['task'] == 'summarization'])} samples")
            print(f"- MCQ Generation: {len(combined_df[combined_df['task'] == 'mcq_generation'])} samples")

        except Exception as e:
            print(f"Error preparing datasets: {e}")
            import traceback
            traceback.print_exc()
            return None, None

        # Verify we have data before proceeding
        if combined_df.empty:
            print("ERROR: Final dataset is empty! Cannot proceed with training.")
            return None, None

        # Initialize the model and check for existing checkpoints
        print("\n==== Initializing model ====")
        model = MultiTaskEducationalModel()

        # Check for existing checkpoints
        latest_checkpoint = None
        start_epoch = 0
        if os.path.exists(CHECKPOINT_DIR):
            checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.endswith('.pt')]
            if checkpoints:
                latest_checkpoint = os.path.join(CHECKPOINT_DIR, sorted(checkpoints)[-1])
                print(f"Found checkpoint at {latest_checkpoint}")
                checkpoint = torch.load(latest_checkpoint, map_location='cuda' if torch.cuda.is_available() else 'cpu')
                model.load_state_dict(checkpoint['model_state_dict'])
                start_epoch = checkpoint.get('epoch', 0) + 1
                print(f"Resuming training from epoch {start_epoch}")

        # FIXED: Prepare data loaders using new method
        print("\n==== Preparing data loaders ====")
        try:
            train_loader, val_loader = model.prepare_data(
                combined_df=combined_df,  # FIXED: Use combined_df instead of separate dataframes
                max_source_len=512,
                max_target_len=256,  # FIXED: Increased for MCQ outputs
                batch_size=BATCH_SIZE,
                val_split=0.1
            )
        except Exception as e:
            print(f"Error preparing data loaders: {e}")
            import traceback
            traceback.print_exc()
            return None, None

        # Train the model
        print("\n==== Starting model training ====")
        training_stats = model.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=EPOCHS,
            start_epoch=start_epoch,
            learning_rate=5e-5,
            warmup_ratio=0.1,
            gradient_accumulation_steps=4,
            max_grad_norm=1.0,
            checkpoint_dir=CHECKPOINT_DIR,
            log_steps=SAVE_STEPS
        )

        # Save training stats and artifacts
        with open(f"{OUTPUT_DIR}/training_stats.json", 'w') as f:
            json.dump(training_stats, f)
        torch.save(model.state_dict(), FINAL_MODEL_PATH)
        model.tokenizer.save_pretrained(f"{OUTPUT_DIR}/tokenizer")
        print(f"Final model and tokenizer saved to {OUTPUT_DIR}")

        # Initialize and save the PDF processor
        processor = EducationalPDFProcessor(
            embedding_model="sentence-transformers/all-mpnet-base-v2",
            max_segment_words=150,
            training_data=combined_df  # Pass the combined dataset
        )
        import pickle
        with open(f"{OUTPUT_DIR}/pdf_processor.pkl", 'wb') as f:
            pickle.dump(processor, f)
        print(f"PDF processor saved to {OUTPUT_DIR}/pdf_processor.pkl")

    # Process PDF and generate output
    print("\n==== Processing PDF Document ====")
    if os.path.exists(PDF_PATH):
        print(f"Processing: {PDF_PATH}")

        # FIXED: Use the corrected function name
        results = run_pdf_inference(model, processor, PDF_PATH)

        print("\n==== Generated Summary ====")
        print("\n" + results["summary"])

        if results["qa_pairs"]:
            print(f"\n==== Multiple Choice Questions ({len(results['qa_pairs'])} total) ====")

            # Display all questions with improved formatting
            for i, item in enumerate(results["qa_pairs"], 1):
                qa = item["qa_data"]
                question = qa['question']

                print(f"\nQuestion {i}: {question}")
                print("\nOptions:")
                for letter, opt in qa["options"].items():
                    print(f"  {letter}. {opt}")
                print(f"\nCorrect Answer: {qa['correct_answer']}")
                print("-" * 80)
        else:
            print("\n==== No Multiple Choice Questions Generated ====")
            print("The content may need manual review for question generation.")

        # Enhanced statistics with quality metrics
        print(f"\n==== Processing Statistics ====")
        print(f"Total Questions Generated: {len(results['qa_pairs'])}")
        print(f"Subject Area Detected: {results['subject']}")

        summary_lines = [line for line in results['summary'].split('\n') if line.strip().startswith('- ')]
        print(f"Summary Key Points: {len(summary_lines)}")

        # Quality assessment
        if results["qa_pairs"]:
            avg_question_length = sum(len(item["qa_data"]["question"].split()) for item in results["qa_pairs"]) / len(results["qa_pairs"])
            avg_answer_length = sum(len(item["qa_data"]["options"][item["qa_data"]["correct_answer"]].split()) for item in results["qa_pairs"]) / len(results["qa_pairs"])

            print(f"Average Question Length: {avg_question_length:.1f} words")
            print(f"Average Answer Length: {avg_answer_length:.1f} words")

            # Question type analysis
            question_types = {}
            for item in results["qa_pairs"]:
                qa = item["qa_data"]
                question = qa['question'].lower()
                if question.startswith('what is') or question.startswith('what does'):
                    q_type = 'Definition/Function'
                elif question.startswith('which of'):
                    q_type = 'Multiple Choice'
                elif question.startswith('how'):
                    q_type = 'Process/Method'
                elif question.startswith('where') or question.startswith('when'):
                    q_type = 'Location/Time'
                else:
                    q_type = 'General'

                question_types[q_type] = question_types.get(q_type, 0) + 1

            print(f"\nQuestion Types Distribution:")
            for q_type, count in question_types.items():
                print(f"  {q_type}: {count}")

    else:
        print(f"Test PDF not found at {PDF_PATH}")

    print("\n==== Processing Completed ====")
    print(f"All outputs saved to: {OUTPUT_DIR}")
    print("Model trained with enhanced MCQ generation capabilities!")
    print("Ready for quiz application integration!")

    return model, processor

if __name__ == "__main__":
    model, processor = main()